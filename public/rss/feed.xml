<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>NotionNext BLOG</title>
        <link>https://www.anjhon.top/</link>
        <description>这是一个由NotionNext生成的站点</description>
        <lastBuildDate>Tue, 26 Mar 2024 10:07:38 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>zh-CN</language>
        <copyright>All rights reserved 2024, AnJhon</copyright>
        <item>
            <title><![CDATA[面向开发者的提示工程]]></title>
            <link>https://www.anjhon.top/article/llms-prompt-for-developer</link>
            <guid>https://www.anjhon.top/article/llms-prompt-for-developer</guid>
            <pubDate>Mon, 18 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[之前写过《面向使用者的提示工程》，主要是面向普通用户，在日常使用大语言模型聊天或对话的时候应该如何书写提示词，来改善大模型输出的效果。而本篇主要是面向开发者，介绍在开发 RAG 类基于大模型的应用时应该如何优化和改善提示词，针对特定任务构造能充分发挥大模型能力的 Prompt 的技巧]]></description>
            <content:encoded><![CDATA[<div id="container" class="max-w-5xl font-normal mx-auto undefined"><main class="notion light-mode notion-page notion-block-31017728fd6c4763a91fe16c3b81264f"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-sync-block notion-block-5b3b24951f334752a6f85c3c6e1f5253"><h2 class="notion-h notion-h1 notion-block-c990752259e0482480cc6259c2e7ce7b" data-id="c990752259e0482480cc6259c2e7ce7b"><span><div id="c990752259e0482480cc6259c2e7ce7b" class="notion-header-anchor"></div><a class="notion-hash-link" href="#c990752259e0482480cc6259c2e7ce7b" title="一、导读"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">一、导读</span></span></h2><div class="notion-text notion-block-ba1cfd23b9c74c76bad48dbf1a5ab99e">之前写过《<a class="notion-link" href="/cac3471fcdd3458399cb4dcbce4684bf">面向使用者的提示工程</a>》，主要是面向普通用户，在日常使用大语言模型聊天或对话的时候应该如何书写提示词，来改善大模型输出的效果。而本篇主要是面向开发者，介绍在开发 RAG 类基于大模型的应用时应该如何优化和改善提示词，<b>针对特定任务构造能充分发挥大模型能力的 Prompt 的技巧</b></div><div class="notion-text notion-block-bcad081359ac4a4aa9603a8859a1293d">整理自：</div><ul class="notion-list notion-list-disc notion-block-e68e691f0a984e2bbc1066e9ee0dabbc"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/datawhalechina/llm-cookbook">面向开发者的大模型手册</a></b></li></ul><ul class="notion-list notion-list-disc notion-block-1b43dbad681941ff813d8d37ed9be196"><li><b>吴恩达老师的 </b><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://learn.deeplearning.ai/courses/chatgpt-prompt-eng/lesson/1/introduction">ChatGPT Prompt Engineering for Developers</a></b><b> 课程</b></li></ul><h2 class="notion-h notion-h1 notion-block-84d275ceb6134f01bc5fe242a80e0f80" data-id="84d275ceb6134f01bc5fe242a80e0f80"><span><div id="84d275ceb6134f01bc5fe242a80e0f80" class="notion-header-anchor"></div><a class="notion-hash-link" href="#84d275ceb6134f01bc5fe242a80e0f80" title="二、提示原则"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">二、提示原则</span></span></h2><h3 class="notion-h notion-h2 notion-block-5679bb707e4349eb9343babb0ac72986" data-id="5679bb707e4349eb9343babb0ac72986"><span><div id="5679bb707e4349eb9343babb0ac72986" class="notion-header-anchor"></div><a class="notion-hash-link" href="#5679bb707e4349eb9343babb0ac72986" title="2.1、原则一：编写清晰、具体的指令"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.1、原则一：编写清晰、具体的指令</b></span></span></h3><h4 class="notion-h notion-h3 notion-block-58385f7cf0fe4d31ba07db1ed126345d" data-id="58385f7cf0fe4d31ba07db1ed126345d"><span><div id="58385f7cf0fe4d31ba07db1ed126345d" class="notion-header-anchor"></div><a class="notion-hash-link" href="#58385f7cf0fe4d31ba07db1ed126345d" title="2.1.1、使用分隔符清晰地表示输入的不同部分"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.1.1、使用分隔符清晰地表示输入的不同部分</b></span></span></h4><div class="notion-text notion-block-85c648a77bd5420aaab407cee1b7d63b"><b>作用：</b></div><ul class="notion-list notion-list-disc notion-block-54b5cfb6ea404159b8511c26ce8d77ca"><li>将不同的部分进行分割，避免混淆</li></ul><ul class="notion-list notion-list-disc notion-block-8b211f3463c4489682bfc40f041c2672"><li>避免词注入（用户输入的文本可能与预设的 prompt 冲突，可能导致可以直接通过输入操控语言模型、或者干扰模型而得出较差的效果）</li></ul><details class="notion-toggle notion-block-589dccf3e20b47f8bc03948d3fec473d"><summary><b>示例：</b></summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">text = f&quot;&quot;&quot;
您应该提供尽可能清晰、具体的指示，以表达您希望模型执行的任务。\
这将引导模型朝向所需的输出，并降低收到无关或不正确响应的可能性。\
不要将写清晰的提示词与写简短的提示词混淆。\
在许多情况下，更长的提示词可以为模型提供更多的清晰度和上下文信息，从而导致更详细和相关的输出。
&quot;&quot;&quot;
# 需要总结的文本内容
prompt = f&quot;&quot;&quot;
把用三个反引号括起来的文本总结成一句话。
```{text}```
&quot;&quot;&quot;</code></pre></div></details><h4 class="notion-h notion-h3 notion-block-72b1719a770f43dfb7cc46b2cbc64273" data-id="72b1719a770f43dfb7cc46b2cbc64273"><span><div id="72b1719a770f43dfb7cc46b2cbc64273" class="notion-header-anchor"></div><a class="notion-hash-link" href="#72b1719a770f43dfb7cc46b2cbc64273" title="2.1.2、寻求结构化的输出"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.1.2、寻求结构化的输出</b></span></span></h4><div class="notion-text notion-block-2a039490e77f402d91646800289f7ebd">有时候我们需要语言模型给我们一些<b>结构化的输出，</b>如JSON、HTML等，而不仅仅是连续的文本。</div><details class="notion-toggle notion-block-2b3fe87828ce43dcb3d8b94f883cc3ec"><summary>示例</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
请生成包括书名、作者和类别的三本虚构的、非真实存在的中文书籍清单，\
并以 JSON 格式提供，其中包含以下键:book_id、title、author、genre。
&quot;&quot;&quot;</code></pre></div></details><h4 class="notion-h notion-h3 notion-block-bf5b64c6ce734fce9edde1bf086540bc" data-id="bf5b64c6ce734fce9edde1bf086540bc"><span><div id="bf5b64c6ce734fce9edde1bf086540bc" class="notion-header-anchor"></div><a class="notion-hash-link" href="#bf5b64c6ce734fce9edde1bf086540bc" title="2.1.3、要求模型检查是否满足条件"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.1.3、要求模型检查是否满足条件</b></span></span></h4><div class="notion-text notion-block-6cb61244da614daca12abc21c3aabff0">如果任务包含不一定能满足的假设（条件），我们可以告诉模型先检查这些假设。</div><details class="notion-toggle notion-block-bb046ec4e0a646c2b5c12356e6cd8a75"><summary>示例</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">text_1 = f&quot;&quot;&quot;
泡一杯茶很容易。首先，需要把水烧开。\
在等待期间，拿一个杯子并把茶包放进去。\
一旦水足够热，就把它倒在茶包上。\
等待一会儿，让茶叶浸泡。几分钟后，取出茶包。\
如果您愿意，可以加一些糖或牛奶调味。\
就这样，您可以享受一杯美味的茶了。
&quot;&quot;&quot;
prompt = f&quot;&quot;&quot;
您将获得由三个引号括起来的文本。\
如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：

第一步 - ...
第二步 - …
…
第N步 - …

如果文本中不包含一系列的指令，则直接写“未提供步骤”。&quot;
\&quot;\&quot;\&quot;{text_1}\&quot;\&quot;\&quot;
&quot;&quot;&quot;</code></pre></div></details><h4 class="notion-h notion-h3 notion-block-86f1966e4639493f972fef8e310abb40" data-id="86f1966e4639493f972fef8e310abb40"><span><div id="86f1966e4639493f972fef8e310abb40" class="notion-header-anchor"></div><a class="notion-hash-link" href="#86f1966e4639493f972fef8e310abb40" title="2.1.4、提供少量示例"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.1.4、提供少量示例</b></span></span></h4><div class="notion-text notion-block-b86e6533fe6346ef8ddc4cfa3f44316a">给模型一两个已完成的样例，让模型了解我们的要求和期望的输出样式</div><details class="notion-toggle notion-block-a41acade2f994b2a8e1ab6a38e5d7412"><summary>示例</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
您的任务是以一致的风格回答问题。

&lt;孩子&gt;: 请教我何为耐心。

&lt;祖父母&gt;: 挖出最深峡谷的河流源于一处不起眼的泉眼；最宏伟的交响乐从单一的音符开始；最复杂的挂毯以一根孤独的线开始编织。

&lt;孩子&gt;: 请教我何为韧性。
&quot;&quot;&quot;</code></pre></div></details><h3 class="notion-h notion-h2 notion-block-d3e4a72217554b41abb493f0bfc7e59a" data-id="d3e4a72217554b41abb493f0bfc7e59a"><span><div id="d3e4a72217554b41abb493f0bfc7e59a" class="notion-header-anchor"></div><a class="notion-hash-link" href="#d3e4a72217554b41abb493f0bfc7e59a" title="2.2、原则二：给模型时间去思考"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.2、原则二：给模型时间去思考</b></span></span></h3><div class="notion-text notion-block-7e52d3a905884bb3b165b8c501e20091">如果让语言模型匆忙给出结论，其结果很可能不准确。例如，若要语言模型推断一本书的主题，仅提供简单的书名和一句简介是不足够的。</div><div class="notion-text notion-block-73d54479a59e40aaa63a771c62294ec1">应通过 Prompt 指引语言模型进行深入思考。可以要求其先列出对问题的各种看法，说明推理依据，然后再得出最终结论。在 Prompt 中添加逐步推理的要求，能让语言模型投入更多时间逻辑思维，输出结果也将更可靠准确。</div><h4 class="notion-h notion-h3 notion-block-48430d76db4447de9bca90544175e6e2" data-id="48430d76db4447de9bca90544175e6e2"><span><div id="48430d76db4447de9bca90544175e6e2" class="notion-header-anchor"></div><a class="notion-hash-link" href="#48430d76db4447de9bca90544175e6e2" title="2.2.1、指定完成任务所需的步骤"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.2.1、指定完成任务所需的步骤</b></span></span></h4><details class="notion-toggle notion-block-799a6f9b94e8421ebec5df18c9d11936"><summary>示例</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">text = f&quot;&quot;&quot;
在一个迷人的村庄里，兄妹杰克和吉尔出发去一个山顶井里打水。\
他们一边唱着欢乐的歌，一边往上爬，\
然而不幸降临——杰克绊了一块石头，从山上滚了下来，吉尔紧随其后。\
虽然略有些摔伤，但他们还是回到了温馨的家中。\
尽管出了这样的意外，他们的冒险精神依然没有减弱，继续充满愉悦地探索。
&quot;&quot;&quot;
# example 1
prompt_1 = f&quot;&quot;&quot;
执行以下操作：
1-用一句话概括下面用三个反引号括起来的文本。
2-将摘要翻译成英语。
3-在英语摘要中列出每个人名。
4-输出一个 JSON 对象，其中包含以下键：english_summary，num_names。

请用换行符分隔您的答案。

Text:
```{text}```
&quot;&quot;&quot;

prompt_2 = f&quot;&quot;&quot;
1-用一句话概括下面用&lt;&gt;括起来的文本。
2-将摘要翻译成英语。
3-在英语摘要中列出每个名称。
4-输出一个 JSON 对象，其中包含以下键：English_summary，num_names。

请使用以下格式：
文本：&lt;要总结的文本&gt;
摘要：&lt;摘要&gt;
翻译：&lt;摘要的翻译&gt;
名称：&lt;英语摘要中的名称列表&gt;
输出 JSON：&lt;带有 English_summary 和 num_names 的 JSON&gt;

Text: &lt;{text}&gt;
&quot;&quot;&quot;</code></pre></div></details><h4 class="notion-h notion-h3 notion-block-9428d8a7d4304da48338624984f8fd31" data-id="9428d8a7d4304da48338624984f8fd31"><span><div id="9428d8a7d4304da48338624984f8fd31" class="notion-header-anchor"></div><a class="notion-hash-link" href="#9428d8a7d4304da48338624984f8fd31" title="2.2.2、指导模型在下结论之前找出一个自己的解法"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2.2.2、指导模型在下结论之前找出一个自己的解法</b></span></span></h4><div class="notion-text notion-block-f937f1ec3f00477cad8c55f8fabe8011">还可以通过明确指导语言模型进行自主思考，来获得更好的效果。</div><div class="notion-text notion-block-cbe0d7f9a38c4815949d0f93d73d1337">假设我们要语言模型判断一个数学问题的解答是否正确，可以在 Prompt 中先要求语言模型自己尝试解决这个问题，思考出自己的解法，然后再与提供的解答进行对比，判断正确性。这种先让语言模型自主思考的方式，能帮助它更深入理解问题，做出更准确的判断。</div><details class="notion-toggle notion-block-11f5322953a44d8f91a8f88e7d6dda7b"><summary>示例</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
请判断学生的解决方案是否正确，请通过如下步骤解决这个问题：

步骤：

    首先，自己解决问题。
    然后将您的解决方案与学生的解决方案进行比较，对比计算得到的总费用与学生计算的总费用是否一致，并评估学生的解决方案是否正确。
    在自己完成问题之前，请勿决定学生的解决方案是否正确。

使用以下格式：

    问题：问题文本
    学生的解决方案：学生的解决方案文本
    实际解决方案和步骤：实际解决方案和步骤文本
    学生计算的总费用：学生计算得到的总费用
    实际计算的总费用：实际计算出的总费用
    学生计算的费用和实际计算的费用是否相同：是或否
    学生的解决方案和实际解决方案是否相同：是或否
    学生的成绩：正确或不正确

问题：

    我正在建造一个太阳能发电站，需要帮助计算财务。 
    - 土地费用为每平方英尺100美元
    - 我可以以每平方英尺250美元的价格购买太阳能电池板
    - 我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元;

    作为平方英尺数的函数，首年运营的总费用是多少。

学生的解决方案：

    设x为发电站的大小，单位为平方英尺。
    费用：
    1. 土地费用：100x美元
    2. 太阳能电池板费用：250x美元
    3. 维护费用：100,000+100x=10万美元+10x美元
    总费用：100x美元+250x美元+10万美元+100x美元=450x+10万美元

实际解决方案和步骤：
&quot;&quot;&quot;</code></pre></div></details><div class="notion-callout notion-gray_background_co notion-block-7edd632f8a0649d0bf2f5b4b8e869875"><div class="notion-page-icon-inline notion-page-icon-span"><span class="notion-page-icon" role="img" aria-label="⚠️">⚠️</span></div><div class="notion-callout-text">大语言模型可能会产生幻觉问题（自行构造出似是而非的细节），由于幻觉信息往往令人无法辨别真伪，开发者必须警惕并尽量避免它的产生。<div class="notion-text notion-block-b8bc1a14ff8c416da501660f18c4fafb">可以先让语言模型直接引用文本中的原句，然后再进行解答。这可以追踪信息来源，降低虚假内容的风险。</div></div></div><h2 class="notion-h notion-h1 notion-block-475a154cbe494ea9b02430a59c53f57d" data-id="475a154cbe494ea9b02430a59c53f57d"><span><div id="475a154cbe494ea9b02430a59c53f57d" class="notion-header-anchor"></div><a class="notion-hash-link" href="#475a154cbe494ea9b02430a59c53f57d" title="三、迭代优化"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">三、迭代优化</span></span></h2><div class="notion-text notion-block-dc8a9b4db0bd41b7904c3f0bd43c425d">以下是一些常见的问题可供参考</div><ul class="notion-list notion-list-disc notion-block-2ff78664e48f48f9acf6e69c3020cdd0"><li>生成文本太长</li><ul class="notion-list notion-list-disc notion-block-2ff78664e48f48f9acf6e69c3020cdd0"><li>优化：使用最多50个词</li><li>问题：精度不会很准，但是接近预设长度，需要多次尝试</li></ul></ul><ul class="notion-list notion-list-disc notion-block-4516e776ecd942e19eea4ad16b3a3f82"><li>细节处理错误：细节信息错误，侧重角度不对等</li><ul class="notion-list notion-list-disc notion-block-4516e776ecd942e19eea4ad16b3a3f82"><li>请对三个反引号之间的评论文本进行概括，最多30个字，并且侧重在快递服务上。</li><li>请对三个反引号之间的评论文本进行概括，最多30个词汇，并且侧重在产品价格和质量上。</li></ul></ul><ul class="notion-list notion-list-disc notion-block-7704aac113b04fa79f6776e448e7fcad"><li>指定输出格式</li><ul class="notion-list notion-list-disc notion-block-7704aac113b04fa79f6776e448e7fcad"><li>优化：在描述之后，包括一个表格，提供产品的尺寸。表格应该有两列。第一列包括尺寸的名称。第二列只包括英寸的测量值。</li></ul></ul><h2 class="notion-h notion-h1 notion-block-0dfc203bf26a40ffaa611cae0cc8ecdd" data-id="0dfc203bf26a40ffaa611cae0cc8ecdd"><span><div id="0dfc203bf26a40ffaa611cae0cc8ecdd" class="notion-header-anchor"></div><a class="notion-hash-link" href="#0dfc203bf26a40ffaa611cae0cc8ecdd" title="四、任务类型"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">四、任务类型</span></span></h2><div class="notion-text notion-block-d40aa3a2916f4086b473390c6dd55dbe">使用上面提到的优化迭代方法进行不断的优化和迭代；</div><h3 class="notion-h notion-h2 notion-block-89714214bab443d298d8270520057b04" data-id="89714214bab443d298d8270520057b04"><span><div id="89714214bab443d298d8270520057b04" class="notion-header-anchor"></div><a class="notion-hash-link" href="#89714214bab443d298d8270520057b04" title="4.1、文本概括"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">4.1、文本概括</span></span></h3><details class="notion-toggle notion-block-4831e38f7e54460f9ed7e47ac74b9f55"><summary>示例：单一文本概括</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = &quot;您的任务是从电子商务网站上生成一个产品评论的简短摘要。&quot;


# 如果只想要提取某一个单独的信息并过滤其他所有的信息可以使用文本提取
prompt = &quot;您的任务是从电子商务网站上的产品评论中提取相关信息。&quot;</code></pre></div></details><details class="notion-toggle notion-block-6165df865abc4f6f9985200a68f12945"><summary>多条文本概括</summary><div><div class="notion-text notion-block-b376bc58fd1847d7aea9fbab3e3fc6e1">for 循环、整合品论、分布式</div></div></details><h3 class="notion-h notion-h2 notion-block-446333612d834c9ca41e50534369f9df" data-id="446333612d834c9ca41e50534369f9df"><span><div id="446333612d834c9ca41e50534369f9df" class="notion-header-anchor"></div><a class="notion-hash-link" href="#446333612d834c9ca41e50534369f9df" title="4.2、推断"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">4.2、推断</span></span></h3><details class="notion-toggle notion-block-bc80bfe12f09459e84623639821743cd"><summary>示例：情感分类</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
以下用三个反引号分隔的产品评论的情感是什么？
用一个单词回答：「正面」或「负面」。
评论文本: ```{lamp_review}```
&quot;&quot;&quot;

prompt = f&quot;&quot;&quot;
识别以下评论的作者表达的情感。包含不超过五个项目。将答案格式化为以逗号分隔的单词列表。
评论文本: ```{lamp_review}```
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-d5fc283435f24e959852038718a9e136"><summary>示例：信息提取</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
从评论文本中识别以下项目：
- 评论者购买的物品
- 制造该物品的公司

评论文本用三个反引号分隔。将你的响应格式化为以 “物品” 和 “品牌” 为键的 JSON 对象。
如果信息不存在，请使用 “未知” 作为值。
让你的回应尽可能简短。

评论文本: ```{lamp_review}```
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-f4b839a10d9e48e694e22c6231fe1580"><summary>示例：信息提取和情感分析</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
从评论文本中识别以下项目：
- 情绪（正面或负面）
- 审稿人是否表达了愤怒？（是或否）
- 评论者购买的物品
- 制造该物品的公司

评论用三个反引号分隔。将你的响应格式化为 JSON 对象，以 “情感倾向”、“是否生气”、“物品类型” 和 “品牌” 作为键。
如果信息不存在，请使用 “未知” 作为值。
让你的回应尽可能简短。
将 “是否生气” 值格式化为布尔值。

评论文本: ```{lamp_review}```
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-19a9959be84144e0b428ec4822599319"><summary>示例：推断主题</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
确定以下给定文本中讨论的五个主题。

每个主题用1-2个词概括。

请输出一个可解析的Python列表，每个元素是一个字符串，展示了一个主题。

给定文本: ```{story}```
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-225cbb40c00d40dda7bf9bdc34218961"><summary>示例：推断主题并判断</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
判断主题列表中的每一项是否是给定文本中的一个话题，

以列表的形式给出答案，每个元素是一个Json对象，键为对应主题，值为对应的 0 或 1。

主题列表：美国航空航天局、当地政府、工程、员工满意度、联邦政府

给定文本: ```{story}```
&quot;&quot;&quot;</code></pre></div></details><h3 class="notion-h notion-h2 notion-block-7ec514a4c60748a6b20a50a420554a3a" data-id="7ec514a4c60748a6b20a50a420554a3a"><span><div id="7ec514a4c60748a6b20a50a420554a3a" class="notion-header-anchor"></div><a class="notion-hash-link" href="#7ec514a4c60748a6b20a50a420554a3a" title="4.3、文本转换"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">4.3、文本转换</span></span></h3><details class="notion-toggle notion-block-d009f3c199f149ed863630d74e78b7cd"><summary>示例：文本翻译</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
将以下中文翻译成西班牙语: \ 
```您好，我想订购一个搅拌机。```
&quot;&quot;&quot;

prompt = f&quot;&quot;&quot;
请将以下文本分别翻译成中文、英文、法语和西班牙语: 
```I want to order a basketball.```
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-2fa934dcc2ac40f785f3ef004d2ee42e"><summary>示例：识别语种</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
请告诉我以下文本是什么语种: 
```Combien coûte le lampadaire?```
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-dd20cb21f1cd4827856132abdf4bbffb"><summary>示例：写作语气与风格调整</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
将以下文本翻译成商务信函的格式: 
```小老弟，我小羊，上回你说咱部门要采购的显示器是多少寸来着？```
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-02459a9cf0ea41f19dbcbf24992831b7"><summary>示例：文件格式转换</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
将以下Python字典从JSON转换为HTML表格，保留表格标题和列名：{data_json}
&quot;&quot;&quot;</code></pre></div></details><details class="notion-toggle notion-block-19477c9b357e44cda89d8d029f458331"><summary>示例：拼写及语法纠正</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;请校对并更正以下文本，注意纠正文本保持原始语种，无需输出原始文本。
如果您没有发现任何错误，请说“未发现错误”。

例如：
输入：I are happy.
输出：I am happy.
```{text[i]}```&quot;&quot;&quot;


prompt = f&quot;校对并更正以下商品评论：```{text}```&quot;</code></pre></div></details><details class="notion-toggle notion-block-d30f49723bbe41ad8bd89f9710e2a61c"><summary>示例：综合案例</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
针对以下三个反引号之间的英文评论文本，
首先进行拼写及语法纠错，
然后将其转化成中文，
再将其转化成优质淘宝评论的风格，从各种角度出发，分别说明产品的优点与缺点，并进行总结。
润色一下描述，使评论更具有吸引力。
输出结果格式为：
【优点】xxx
【缺点】xxx
【总结】xxx
注意，只需填写xxx部分，并分段输出。
将结果输出成Markdown格式。
```{text}```
&quot;&quot;&quot;</code></pre></div></details><h3 class="notion-h notion-h2 notion-block-11e7f2b275c34286add33fa8f01415f6" data-id="11e7f2b275c34286add33fa8f01415f6"><span><div id="11e7f2b275c34286add33fa8f01415f6" class="notion-header-anchor"></div><a class="notion-hash-link" href="#11e7f2b275c34286add33fa8f01415f6" title="4.4、 文本扩展"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">4.4、 文本扩展</span></span></h3><details class="notion-toggle notion-block-5fde1827a2b9435c9db30b86314aa288"><summary>示例：定制客户邮件</summary><div><div class="notion-text notion-block-2b9aa34b0fcf4679bd80427c2caf32e6"><b>根据客户的评价和其中的情感倾向，使用大语言模型针对性地生成回复邮件</b>。</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">prompt = f&quot;&quot;&quot;
你是一位客户服务的AI助手。
你的任务是给一位重要客户发送邮件回复。
根据客户通过“```”分隔的评价，生成回复以感谢客户的评价。提醒模型使用评价中的具体细节
用简明而专业的语气写信。
作为“AI客户代理”签署电子邮件。
客户评论：
```{review}```
评论情感：{sentiment}
&quot;&quot;&quot;</code></pre></div></details><h3 class="notion-h notion-h2 notion-block-5ed1fd8e570641cf822caa8bf8c6a47b" data-id="5ed1fd8e570641cf822caa8bf8c6a47b"><span><div id="5ed1fd8e570641cf822caa8bf8c6a47b" class="notion-header-anchor"></div><a class="notion-hash-link" href="#5ed1fd8e570641cf822caa8bf8c6a47b" title="4.5、聊天对话"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">4.5、聊天对话</span></span></h3><div class="notion-text notion-block-dcfce6f85da949378157bcea9267d86f">像 ChatGPT 这样的聊天模型实际上是组装成以一系列消息作为输入，并返回一个模型生成的消息作为输出的。</div><details class="notion-toggle notion-block-175dceb286d74c0392df591fe9fe88d7"><summary>示例：设置角色</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">messages =  [  
{&#x27;role&#x27;:&#x27;system&#x27;, &#x27;content&#x27;:&#x27;你是个友好的聊天机器人。&#x27;},    
{&#x27;role&#x27;:&#x27;user&#x27;, &#x27;content&#x27;:&#x27;Hi, 我是Isa。&#x27;}  ]</code></pre></div></details><details class="notion-toggle notion-block-617589be03d24eb786177acc77dad455"><summary>示例：构建构建上下文</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">messages =  [  
{&#x27;role&#x27;:&#x27;system&#x27;, &#x27;content&#x27;:&#x27;你是个友好的聊天机器人。&#x27;},
{&#x27;role&#x27;:&#x27;user&#x27;, &#x27;content&#x27;:&#x27;Hi, 我是Isa&#x27;},
{&#x27;role&#x27;:&#x27;assistant&#x27;, &#x27;content&#x27;: &quot;Hi Isa! 很高兴认识你。今天有什么可以帮到你的吗?&quot;},
{&#x27;role&#x27;:&#x27;user&#x27;, &#x27;content&#x27;:&#x27;是的，你可以提醒我, 我的名字是什么?&#x27;}  ]</code></pre></div></details><details class="notion-toggle notion-block-05471e8cb54e4a35907d9c7509b2428f"><summary>示例：订餐机器人</summary><div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">context = [{&#x27;role&#x27;:&#x27;system&#x27;, &#x27;content&#x27;:&quot;&quot;&quot;
你是订餐机器人，为披萨餐厅自动收集订单信息。
你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。
最后需要询问是否自取或外送，如果是外送，你要询问地址。
最后告诉顾客订单总金额，并送上祝福。

请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。
你的回应应该以简短、非常随意和友好的风格呈现。

菜单包括：

菜品：
意式辣香肠披萨（大、中、小） 12.95、10.00、7.00
芝士披萨（大、中、小） 10.95、9.25、6.50
茄子披萨（大、中、小） 11.95、9.75、6.75
薯条（大、小） 4.50、3.50
希腊沙拉 7.25

配料：
奶酪 2.00
蘑菇 1.50
香肠 3.00
加拿大熏肉 3.50
AI酱 1.50
辣椒 1.00

饮料：
可乐（大、中、小） 3.00、2.00、1.00
雪碧（大、中、小） 3.00、2.00、1.00
瓶装水 5.00
&quot;&quot;&quot;} ]


messages =  context.copy()
messages.append(
{&#x27;role&#x27;:&#x27;system&#x27;, &#x27;content&#x27;:
&#x27;&#x27;&#x27;创建上一个食品订单的 json 摘要。\
逐项列出每件商品的价格，字段应该是 1) 披萨，包括大小 2) 配料列表 3) 饮料列表，包括大小 4) 配菜列表包括大小 5) 总价
你应该给我返回一个可解析的Json对象，包括上述字段&#x27;&#x27;&#x27;},    
)</code></pre></div></details></div></main></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GitHub更新上游仓库的指定版本]]></title>
            <link>https://www.anjhon.top/article/github-update</link>
            <guid>https://www.anjhon.top/article/github-update</guid>
            <pubDate>Mon, 31 Jul 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[在此之前，更新仓库是可以选定上游仓库的某个tag来进行更新合并代码的，但是现在无论怎么找都找不到了（网页版和桌面版都找了没有了），这么方便的功能，被取消了就很无语。]]></description>
            <content:encoded><![CDATA[<div id="container" class="max-w-5xl font-normal mx-auto undefined"><main class="notion light-mode notion-page notion-block-bd2789fb84ad4b66a58f204907e8248c"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-text notion-block-9493d1ac8661453496bd07c28bd65771">我前两天在更新仓库时，发现之前很好用的一个功能不见了（盲猜应该是被取消了）。在此之前，更新仓库是可以选定上游仓库的某个tag来进行更新合并代码的，但是现在无论怎么找都找不到了（网页版和桌面版都找了没有了），这么方便的功能，被取消了就很无语。</div><div class="notion-text notion-block-361f11b5b27b44e9b1494fd92415e63e">以NotionNext为例，这个仓库更新比较频繁，大佬发布了很多版本（用tag来标记的），之前在升级时，无论是在网页版中新建分支，还是在GitHub Desktop中选择上游分支合并，都能选择指定的tag来升级。</div><div class="notion-text notion-block-dfd30d0a92644c1b824f3e767f92a21d">比如在网页端新建分支的时候，之前是可以选择tag的，但是现在默认只能选branch，如下</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-661445c13e2d4d8798f121152b6cc0af"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:576px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F8b92af27-7716-428b-a7e3-cc3fa895b2bc%2FUntitled.png?table=block&amp;id=661445c1-3e2d-4d87-98f1-21152b6cc0af" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-blank notion-block-d976d39a4b424c84812a908017cf4c46"> </div><div class="notion-text notion-block-38bb30499b07421584e1fdff57fcfb57">在我琢磨了很久之后，想到了一个曲线救国的方法，比之前的要麻烦，但是也能达到相同的效果，其实就是实现版本回退</div><div class="notion-text notion-block-76042313f2c64114aa8db763983e8893"><b>具体步骤如下：</b></div><div class="notion-text notion-block-79ee9c2c28f94eebba3a71b08caf530f">1、先在网页上新建分支，分支的来源是上游仓库的main分支</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-07251cd20ace4f3ea2b5b30df7e88661"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:528px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F596d78d6-616c-47ca-9ec1-53bbc254662a%2FUntitled.png?table=block&amp;id=07251cd2-0ace-4f3e-a2b5-b30df7e88661" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-c8697e8c1d1c439397c229f8aad45a49">2、克隆到本地，使用GitHub Desktop打开项目文件夹（或者直接在网页上点击<b>Open with GitHub Desktop</b>）</div><div class="notion-text notion-block-467afa24f2684de1b7622144ff67d133">3、切换分支为我们第一步创建的最新分支</div><div class="notion-text notion-block-9bf5a4de923249a2b9fe6ecad1d732c8">4、在历史记录中查找我们想更新的版本提交记录</div><div class="notion-text notion-block-4caf7a42b5594415a043ad31979236e0">5、右键——&gt;Create Branch from Commit</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-d8aa1bf4755b412aa71b70d392d2d8f3"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:528px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://cdn.jsdelivr.net/gh/An-Jhon/image_bed/202403251838529.png" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-4d8d00f23f534420bb19f92ed2704e9c">6、将这个新分支与要更新的分支合并（在GitHub Desktop中实现）</div><div class="notion-text notion-block-affe166468bd48f4a12cefa51ef7f8fd">GitHub Desktop的合并教程可以看我之前的文章：<a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.anjhon.top/article/d13232f8-ea60-4a69-87dd-81ef5e9da34e"><b>GitHub Part1 克隆、同步、代码冲突</b></a></div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-c52b10da1b9e4d088c0f59a2d74dafb4"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:528px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3c3e5a70-19d8-4c47-a57d-85228002742a%2FUntitled.png?table=block&amp;id=c52b10da-1b9e-4d08-8c0f-59a2d74dafb4" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-6e16ac0360c34e55984dd3aec457656a">7、同步内容到 GitHub</div></main></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[《娱乐至死》]]></title>
            <link>https://www.anjhon.top/article/reading-notes-amusing-ourseles-to-death</link>
            <guid>https://www.anjhon.top/article/reading-notes-amusing-ourseles-to-death</guid>
            <pubDate>Fri, 22 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[有两种方法可以让文化精神枯萎，一种是奥威尔式的——文化成为一个监狱，另一种是赫胥黎式的——文化成为一场滑稽戏。
——尼尔·波兹曼
《娱乐至死》初版于1985年，是尼尔·波兹曼的代表作之一。]]></description>
            <content:encoded><![CDATA[<div id="container" class="max-w-5xl font-normal mx-auto undefined"><main class="notion light-mode notion-page notion-block-fc3a58b2db174abf9c54e4272c6f946f"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-sync-block notion-block-fb8a4309659547d086836c728aac4ccc"><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-60fc5bba56b045d49fc3fbd9a6956afe"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://cdn.jsdelivr.net/gh/An-Jhon/image_bed/202403222038238.png" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-blank notion-block-e6f88214fcbd429da7320babddfdcc71"> </div><div class="notion-text notion-block-ad416bd6364e4e8bbc68c246b06e3cfb">商品的质量和用途在展示商品的技巧面前似乎是无足轻重的</div><div class="notion-text notion-block-43a7c67c7ef0446eb1ef9f189156d560">我们认识到的自然、智力、人类动机或思考，并不是他们本来的面目，而是他们在语言中的表现形式。我们的语言即媒介，我们的媒介即隐喻，我们的隐喻创造了我们的文化的内容。</div><div class="notion-text notion-block-bfc030dc888345189bb1e6330197d40e">不管一种媒介原来的语境是怎样的，它都有能力越过这个语境并延伸到新的未知的语境中。</div><div class="notion-text notion-block-d9013c7aca6649e68695c58baf3e71c6">关于真理的看法在随着文化媒介的变化而不断改变，尼采说，任何哲学都是每个阶段生活的哲学。我们还应该加一句，任何认识论都是某个媒介发展的认识论。真理和时间一样，是人通过他自己发明的交流技术同自己进行对话的产物。</div><div class="notion-text notion-block-646c590a35d546278374f653eddee392">对于真理的认识是同表达方式密切相关的，它不能也从来没有毫无修饰地存在</div><div class="notion-text notion-block-b3fa0dcb893a4acc8e5b0d0f7b2436a2">如果只拘泥于耳熟能详的专业术语，就会丧失对事物进行宏观全面认识的能力</div><div class="notion-text notion-block-ff554e3b6b98441c80c9609b619b2637">我们的问题不在于电视为我们展示具有娱乐性的内容，而在于所有内容都以娱乐的方式展现出来。娱乐是电视上所有话语的超意识形态</div><div class="notion-text notion-block-086598ca066d43388fa82d91714cc95a">电视是我们文化中存在的、了解文化的最主要的方式。于是电视中表现的世界便成了这个世界如何存在的模型。娱乐不仅仅在电视上成为所有话语的象征，在电视之外，这种象征仍然统治着一切。</div><div class="notion-text notion-block-9a24fbde2adb4260acb6f54a1d66f559">电视为真实性提供了一种新的定义：讲述者的可信度决定了事件的真实性</div><div class="notion-text notion-block-a0648edf11eb43deb1e57600e44d460a">当新闻被包装成一种娱乐形式时，它就不可避免的气到了蒙蔽作用</div><div class="notion-text notion-block-08c1ec4b73f74d4f9e5d9c482e65dd0f">谎言没有被定义为真理，真理也没有被定义为谎言。真正发生的是公众已经适应了没有连贯性的世界，并且已经被娱乐得麻木不仁了。</div><div class="notion-text notion-block-a8240a9ac86b43e6888c415833fe47ba">历史已经证明，一个文化不会因为假消息和错误观点而灭亡，但历史从来没有证明过，一个自认为可以在 22 分钟内评价整个世界的文化还会有生存的能力，除非，新闻的价值取决于他能带来多少笑声</div><div class="notion-text notion-block-1e6b13f3a8514d21bbbe7fb902e8a05b">电视最大的长处是它让具体的形象进入我们的心里，而不是让抽象的概念留在我们脑海中。</div><div class="notion-text notion-block-e55da3f99cc84ef8956b8261381eb4b9">真正的危险不在于宗教已经成为电视节目的内容，而在于电视节目可能会成为宗教的内容</div><div class="notion-text notion-block-7de2bcc7b7924bef81ea0645f987853c">电视上的陈述性论题和相貌平平的人一样稀少</div><div class="notion-text notion-block-3816f89e2692439c80448a8e8e96f55b">电视广告的对象不是产品的品质，而是那些产品消费真的品质</div><div class="notion-text notion-block-1677ce56a3d046e9ada098e51f119531">广告商需要知道的不是产品有什么好处，而是购买者有什么问题。于是企业开支的重心从产品开发专项了市场调查。电视广告把企业从生产有价值的产品引向了设法使消费者感觉产品有价值</div><div class="notion-text notion-block-e3126dcbdb6e43f48e7e8312165bd942">导致历史消失的是人们事不关己的态度，而不是他们的固执和无知</div><div class="notion-text notion-block-71ceaa42cbcc4f8ea678b65c146f5247">我们似乎知道过去 24 小时里发生的任何事情，而对过去 60 个世纪或 60 年里发生的事却知之甚少</div><div class="notion-text notion-block-6376a32bf7054311b8570f180b7ea44a">我们这个时代的特征是“拒绝记忆”，或许我们不是拒绝记忆，也不是认为历史不值得记忆，问题的症结在于我们已经被改造得不会记忆了</div><div class="notion-text notion-block-dcd6abefdd6742beab723a746d31ae2b">我们要担心的是信息过剩，而不是信息限制；在信息洪流面前，我们根本无力保护自己。</div><div class="notion-text notion-block-152a032cb9014e00959d7723b3facc58">理性只有在感情的肥沃土壤里才能得到最好的培养</div><div class="notion-text notion-block-84ffc990ee1941cc84f9c67d6991ef9f">从电视上获得的意义往往是一些具体的片段，不具备推论性（忘得比较快），而从阅读中获得的意义往往和我们原来储存的知识相关，所以具备较强的推论性；所以通过看电视（或现在的短视频）并不能有效的提高学习效率</div><div class="notion-text notion-block-e60766fe82664bfbad189ac929107d51">电视最大的好处是为人们提供存粹的娱乐，最糟糕的用途是涉足严肃的话题模式（新闻、政治、科学、教育、商业、宗教）然后给他们换上娱乐的包装</div><div class="notion-text notion-block-7525cc1e799d45849a73be320eabd546">只有深刻而持久地意识到信息的结构和效应，消除对媒介的神秘感，我们才有可能对电视、或电脑、或其他的媒介获得某种程度的控制。但我们如何培养这种媒介意识呢？通过学校；将怎样利用电视或电脑来控制教育的问题转变为我们怎样利用教育来控制电视或电脑上</div><div class="notion-text notion-block-352ba01c67234d21b19980cde4a6f5cb">人们感到痛苦的不是他们用笑声代替了思考，而是他们不知道自己为什么小以及为什么不思考</div></div></main></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[那些GitHub上开源项目]]></title>
            <link>https://www.anjhon.top/article/github-open-projects</link>
            <guid>https://www.anjhon.top/article/github-open-projects</guid>
            <pubDate>Mon, 14 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[GitHub作为全球最大的程序员交友网站，自然有很多优秀的开源项目。本篇主要是整理一些我用过觉得好用的或想用的一些GitHub开源项目，会进行持续更新。]]></description>
            <content:encoded><![CDATA[<div id="container" class="max-w-5xl font-normal mx-auto undefined"><main class="notion light-mode notion-page notion-block-313ec2608f30412eb61705953ca5c9e3"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-fe646becb1d64c31a8e6eeceaba2daf6" data-id="fe646becb1d64c31a8e6eeceaba2daf6"><span><div id="fe646becb1d64c31a8e6eeceaba2daf6" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fe646becb1d64c31a8e6eeceaba2daf6" title="一、开源Mac软件"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">一、开源Mac软件</span></span></h2><div class="notion-text notion-block-c905cc4887d84cb0a8c50ab96fe94622">1、外接显示器音量、亮度调节<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/MonitorControl/MonitorControl" class="notion-external notion-external-block notion-row notion-block-51007a476e4644b3a6df4a0ab97826a1"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">MonitorControl</div><div class="notion-external-subtitle"><span>MonitorControl</span><span> • </span><span>Updated <!-- -->Aug 31, 2023</span></div></div></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/alin23/lunar" class="notion-external notion-external-block notion-row notion-block-7b53f6769097445dbbd4115297803e4c"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">Lunar</div><div class="notion-external-subtitle"><span>alin23</span><span> • </span><span>Updated <!-- -->Aug 30, 2023</span></div></div></a></div></div><div class="notion-text notion-block-cba0cfc12d124285955e030ea7d7417c">2、翻转、平滑鼠标<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Caldis/Mos" class="notion-external notion-external-block notion-row notion-block-06cc90c0e7474da693ada50d6ff074b1"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">Mos</div><div class="notion-external-subtitle"><span>Caldis</span><span> • </span><span>Updated <!-- -->Aug 30, 2023</span></div></div></a></div></div><div class="notion-text notion-block-73eb234d84e24e8aa44765847e50c90a">3、功能强大的视频播放器<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/iina/iina" class="notion-external notion-external-block notion-row notion-block-2a42e4b9ea754b0791fc099451228c5e"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">iina</div><div class="notion-external-subtitle"><span>iina</span><span> • </span><span>Updated <!-- -->Aug 31, 2023</span></div></div></a></div></div><div class="notion-text notion-block-6c5f6e1dc38245eca7dbea3f5defc020">4、畅听全网的音乐播放器<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/listen1/listen1_desktop" class="notion-external notion-external-block notion-row notion-block-a16994cc785148eb9f62b742ce992730"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">listen1_desktop</div><div class="notion-external-subtitle"><span>listen1</span><span> • </span><span>Updated <!-- -->Aug 30, 2023</span></div></div></a></div></div><div class="notion-text notion-block-391f54ccbee8437b8eec9c456e9c364e">5、音量分开控制<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kyleneideck/BackgroundMusic" class="notion-external notion-external-block notion-row notion-block-73dde5e3a4684cb0aaa9424e4843f2d7"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">BackgroundMusic</div><div class="notion-external-subtitle"><span>kyleneideck</span><span> • </span><span>Updated <!-- -->Aug 31, 2023</span></div></div></a></div></div><div class="notion-text notion-block-5b92f96ae9ca4a8dacce85d4c26a5136">6、功能丰富的微信助手<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/TKkk-iOSer/WeChatPlugin-MacOS" class="notion-external notion-external-block notion-row notion-block-6870eeff805647be97783b416608bb78"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">WeChatPlugin-MacOS</div><div class="notion-external-subtitle"><span>TKkk-iOSer</span><span> • </span><span>Updated <!-- -->Aug 31, 2023</span></div></div></a></div></div><div class="notion-text notion-block-d3e8664dfb02454887dc82185170955f">7、书籍管理器<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kovidgoyal/calibre" class="notion-external notion-external-block notion-row notion-block-db2f3c1491054e84aaa26451ae692c40"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">calibre</div><div class="notion-external-subtitle"><span>kovidgoyal</span><span> • </span><span>Updated <!-- -->Aug 31, 2023</span></div></div></a></div></div><div class="notion-text notion-block-fb52caef9075481ebe07eba91695395c">其他<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/iCHAIT/awesome-macOS" class="notion-external notion-external-block notion-row notion-block-ac2191d3392d477cbc9721206a3175a5"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">awesome-macOS</div><div class="notion-external-subtitle"><span>iCHAIT</span><span> • </span><span>Updated <!-- -->Aug 30, 2023</span></div></div></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/serhii-londar/open-source-mac-os-apps" class="notion-external notion-external-block notion-row notion-block-30398b127e30413bbb09f77b0b1b4585"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">open-source-mac-os-apps</div><div class="notion-external-subtitle"><span>serhii-londar</span><span> • </span><span>Updated <!-- -->Aug 30, 2023</span></div></div></a><div class="notion-row"><a target="_blank" rel="noopener noreferrer" class="notion-bookmark notion-block-293c7898edb24702a0ba4e28d48b4b3e" href="https://www.howie6879.com/post/2023/01_my_awesome_mac_soft/"><div><div class="notion-bookmark-title">我的macOS常用软件清单 - 老胡的储物柜</div><div class="notion-bookmark-description">我的macOS常用软件清单</div><div class="notion-bookmark-link"><div class="notion-bookmark-link-icon"><img src="https://www.howie6879.com/favicon.ico" alt="我的macOS常用软件清单 - 老胡的储物柜" loading="lazy" decoding="async"/></div><div class="notion-bookmark-link-text">https://www.howie6879.com/post/2023/01_my_awesome_mac_soft/</div></div></div></a></div></div></div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-a426b15471e147e0b2fe8005c32f8df0" data-id="a426b15471e147e0b2fe8005c32f8df0"><span><div id="a426b15471e147e0b2fe8005c32f8df0" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a426b15471e147e0b2fe8005c32f8df0" title="二、奇奇怪怪项目"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">二、奇奇怪怪项目</span></span></h2><div class="notion-text notion-block-f54e302a58a84362be20ee6e13c231e1">1、朋友圈集赞<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/TransparentLC/WechatMomentScreenshot" class="notion-external notion-external-block notion-row notion-block-98af6be72aad4c009c2a2e576fab4d1c"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">WechatMomentScreenshot</div><div class="notion-external-subtitle"><span>TransparentLC</span><span> • </span><span>Updated <!-- -->Aug 30, 2023</span></div></div></a></div></div><div class="notion-text notion-block-28e672c149c7466fbbb68e06167f8046">2、骚话生成器<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/disksing/sao-gen-gen" class="notion-external notion-external-block notion-row notion-block-d65aec5503b749cba20309df31e80fc7"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">sao-gen-gen</div><div class="notion-external-subtitle"><span>disksing</span><span> • </span><span>Updated <!-- -->Aug 4, 2023</span></div></div></a></div></div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-1f3c55f4981449558ab5c44fe6b2bc89" data-id="1f3c55f4981449558ab5c44fe6b2bc89"><span><div id="1f3c55f4981449558ab5c44fe6b2bc89" class="notion-header-anchor"></div><a class="notion-hash-link" href="#1f3c55f4981449558ab5c44fe6b2bc89" title="三、专业性项目"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">三、专业性项目</span></span></h2><ol start="1" class="notion-list notion-list-numbered notion-block-060fa8e054874f32949c83013b753fb7"><li>用于解释性数学视频的动画引擎</li><ol class="notion-list notion-list-numbered notion-block-060fa8e054874f32949c83013b753fb7"><a target="_blank" rel="noopener noreferrer" href="https://github.com/3b1b/manim" class="notion-external notion-external-block notion-row notion-block-01ec8fdfa9644211b27f5136a0908320"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">manim</div><div class="notion-external-subtitle"><span>3b1b</span><span> • </span><span>Updated <!-- -->Mar 9, 2024</span></div></div></a></ol></ol><div class="notion-text notion-block-c461ac23b9ce4b89b72f4b531cd791f7">2、截图转换成前端代码<div class="notion-text-children"><a target="_blank" rel="noopener noreferrer" href="https://github.com/abi/screenshot-to-code" class="notion-external notion-external-block notion-row notion-block-80d2a6bc6b844b9cbf831cf8eaea498d"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">screenshot-to-code</div><div class="notion-external-subtitle"><span>abi</span><span> • </span><span>Updated <!-- -->Mar 19, 2024</span></div></div></a></div></div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-41907532b75c4aeea5023014b3a81bb7" data-id="41907532b75c4aeea5023014b3a81bb7"><span><div id="41907532b75c4aeea5023014b3a81bb7" class="notion-header-anchor"></div><a class="notion-hash-link" href="#41907532b75c4aeea5023014b3a81bb7" title="四、学习类项目"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">四、学习类项目</span></span></h2><div class="notion-text notion-block-0617adebf92643be98b9985c1964ac40">1、推荐系统学习</div><a target="_blank" rel="noopener noreferrer" href="https://github.com/datawhalechina/fun-rec" class="notion-external notion-external-block notion-row notion-block-e6ef4932188e40229764599b3eec58ac"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">fun-rec</div><div class="notion-external-subtitle"><span>datawhalechina</span><span> • </span><span>Updated <!-- -->Mar 20, 2024</span></div></div></a></main></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Transformer]]></title>
            <link>https://www.anjhon.top/article/machine-learning-transformer</link>
            <guid>https://www.anjhon.top/article/machine-learning-transformer</guid>
            <pubDate>Tue, 19 Mar 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Transformer 是由谷歌于 2017 年在 Attention Is All You Need 中首次提出。Transformer的提出在很大程度上改变了自然语言处理领域的局面，取代了传统的循环神经网络和卷积神经网络在语言模型、机器翻译等任务中的地位。]]></description>
            <content:encoded><![CDATA[<div id="container" class="max-w-5xl font-normal mx-auto undefined"><main class="notion light-mode notion-page notion-block-ddacc809371b4411a26b62793ceb70c5"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-sync-block notion-block-5cedc3384c72470cb7e850fa7461c61e"><div class="notion-table-of-contents notion-gray notion-block-e0a314453415428f8ad1b2162ccfacbe"></div><h2 class="notion-h notion-h1 notion-block-6f9fe22f588245a5a8425ac5d977c46f" data-id="6f9fe22f588245a5a8425ac5d977c46f"><span><div id="6f9fe22f588245a5a8425ac5d977c46f" class="notion-header-anchor"></div><a class="notion-hash-link" href="#6f9fe22f588245a5a8425ac5d977c46f" title="一、导读"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">一、导读</span></span></h2><div class="notion-text notion-block-1ad3c67e99e74442b7abe57b1c071e3f">Transformer 是由谷歌于 2017 年在 <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a> 中首次提出。Transformer的提出在很大程度上改变了自然语言处理领域的局面，取代了传统的循环神经网络和卷积神经网络在语言模型、机器翻译等任务中的地位。</div><div class="notion-text notion-block-aa1c69b0aaa34d1d8e87eb84f1365d43">Transformer 的发展历程可以简单概括为：</div><ul class="notion-list notion-list-disc notion-block-85ea4a8286234e28b8d3d3b98db18368"><li>2017 年 6 月首次提出Transformer</li></ul><ul class="notion-list notion-list-disc notion-block-6dcf42a5c8bb4b4aa77d1508409b9ff7"><li>2018 年 2 月 OpenAI 推出 CPT-1 模型（基于 Transformer 的 Decoder 结构）</li></ul><ul class="notion-list notion-list-disc notion-block-d687ff5be1b146ca861af38141856420"><li>2018 年 10 月 谷歌推出 BERT 模型（基于 Transformer 的 Encoder 结构）</li></ul><ul class="notion-list notion-list-disc notion-block-f4f5836da48a4ef08a66983768ab9bdd"><li>2019 年 10 月谷歌推出 T5 模型（基于 Transformer 的 Encoder-Decoder 结构）</li></ul><ul class="notion-list notion-list-disc notion-block-54179ce205854e5580042d508cd48de4"><li>2022 年 11 月 Chat-GPT(基于 GPT3.5) 问世</li></ul><ul class="notion-list notion-list-disc notion-block-ac44a711a15b449ca29eef1ab13a3674"><li>2023 年 3 月 GPT4 问世</li></ul><ul class="notion-list notion-list-disc notion-block-a0090210a8fb4bfdaa90a7379ff0868d"><li>……</li></ul><h2 class="notion-h notion-h1 notion-block-71110b16896e486dacad57edaae78b90" data-id="71110b16896e486dacad57edaae78b90"><span><div id="71110b16896e486dacad57edaae78b90" class="notion-header-anchor"></div><a class="notion-hash-link" href="#71110b16896e486dacad57edaae78b90" title="二、Transformer详解"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">二、Transformer详解</span></span></h2><div class="notion-text notion-block-9a5b1ae697b74965b7cb9cd15b310b1b">转载自：<b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://wmathor.com/index.php/archives/1438/">Transformer 详解</a></b></div><figure class="notion-asset-wrapper notion-asset-wrapper-embed notion-block-9313899aecf8481ab5e82ef2d168daff"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:930px"><iframe class="notion-asset-object-fit" src="https://wmathor.com/index.php/archives/1438/" title="iframe embed" frameBorder="0" allowfullscreen="" loading="lazy" scrolling="auto"></iframe></div></figure><h2 class="notion-h notion-h1 notion-block-01aa56bffebb44dcae416fc772db576c" data-id="01aa56bffebb44dcae416fc772db576c"><span><div id="01aa56bffebb44dcae416fc772db576c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#01aa56bffebb44dcae416fc772db576c" title="三、补充理解"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">三、补充理解</span></span></h2><h3 class="notion-h notion-h2 notion-block-59231af521e54d998bd15a67d5c30fbd" data-id="59231af521e54d998bd15a67d5c30fbd"><span><div id="59231af521e54d998bd15a67d5c30fbd" class="notion-header-anchor"></div><a class="notion-hash-link" href="#59231af521e54d998bd15a67d5c30fbd" title="自注意力机制"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">自注意力机制</span></span></h3><div class="notion-text notion-block-823952c3162b47198251fedd14555982"><b>原理</b></div><div class="notion-text notion-block-e7f107923a9c4e5785283a7348dbf4c8">注意力模型借鉴了人类的注意力机制：人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。同样的，Attention机制可以理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息。</div><div class="notion-text notion-block-5930840782ef44c7b264052b92566813">在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。所以<b>它关注的是输入的数据中哪些是和输出（标签）更相关的</b>。</div><div class="notion-text notion-block-b2b6e9f28c704347ae8e98e17127432f">在Transfoemer 框架中，使用的是 Self Attention 机制，<b>它关注的是输入数据内部元素之间的相关性</b>，因此它的计算结果表示的是输入文本内部各个词（token）之间的相关性，这些相关性在语法上表示的可能是句法特征（有一定距离的短语结构）或者语义特征（指示代词的指代对象）。 而与 RNN 和 LSTM 比起来它更容易捕获句子中<b>长距离</b>的相互依赖的特征。</div><div class="notion-text notion-block-c04460da31194cdda93aa56d024c623f"><b>计算</b></div><div class="notion-text notion-block-a24628c1da5f46b9b00b779a2c4dc942">对于输入的句子 X，通过 Word Embedding 得到该句子中每个字的字向量，同时通过 Positional Encoding 得到所有字的位置向量，将其相加（维度相同，可以直接相加），得到该字真正的向量表示。第 t 个字的向量记作 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>；定义三个矩阵 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> （这三个矩阵一般是随机初始化得到的）使用这三个矩阵与输入 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 相乘，得到<b>查询矩阵 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b> ，键矩阵 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b> ，值矩阵 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b> ；计算公式如下：</b></div><span role="button" tabindex="0" class="notion-equation notion-equation-block"><span></span></span><div class="notion-callout notion-gray_background_co notion-block-f019ace8c1074ab9aa8372ca26a187ec"><div class="notion-page-icon-inline notion-page-icon-span"><span class="notion-page-icon" role="img" aria-label="💡">💡</span></div><div class="notion-callout-text">补充<ol start="1" class="notion-list notion-list-numbered notion-block-d2e37329ac3848e48e6efd18ef3888ba"><li>先从点乘的物理意义说，两个向量的点乘表示两个向量的相似度。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-7a105d8cbaca4474bbec14e95bad21b8"><li>Q，K，V物理意义上是一样的，都表示同一个句子中不同token组成的矩阵。矩阵中的每一行，是表示一个token的word embedding向量。假设一个句子&quot;Hello, how are you?&quot;长度是6，embedding维度是300，那么Q，K，V都是(6, 300)的矩阵</li></ol></div></div><div class="notion-text notion-block-a68b1f979d9e46a6a5b264c9b5ed5675">简单的说，K和Q的点乘是为了计算一个句子中每个token相对于句子中其他token的相似度，这个相似度可以理解为attetnion score，关注度得分。比如说 &quot;Hello, how are you?&quot;这句话，当前token为”Hello&quot;的时候，我们可以知道”Hello“对于” , “, &quot;how&quot;, &quot;are&quot;, &quot;you&quot;, &quot;?&quot;这几个token对应的关注度是多少。有了这个attetnion score，可以知道处理到”Hello“的时候，模型在关注句子中的哪些token。</div><div class="notion-text notion-block-4a4a21fd2f1d4d1a8a45bfb1fe4fad2a">这个attention score是一个(6, 6)的矩阵。每一行代表每个token相对于其他token的关注度。比如说上图中的第一行，代表的是Hello这个单词相对于本句话中的其他单词的关注度。</div><div class="notion-text notion-block-9d219ea2c1d74434acdbd323d315b1f4">公式中计算了K和Q的点乘之后还除以了<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> （ <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 是矩阵Q、K 的列数，即向量的纬度），原因是原始分数通过点积操作计算得到，点积的结果会随着向量的维度增大而增大，而过大的值会导致模型不稳定，所通过除以<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 的方式来对原始分数进行缩放，从而使模型更加稳定收敛，并且减少梯度消失或梯度爆炸的风险。最后添加softmax只是为了对关注度进行归一化。</div><div class="notion-text notion-block-0e6f513442fe4650ad0246c7c51fb405">虽然有了attention score矩阵，但是这个矩阵是经过各种计算后得到的，已经很难表示原来的句子了。然而V还代表着原来的句子，所以我们拿这个attention score矩阵与V相乘，得到的是一个加权后结果。也就是说，原本V里的各个单词只用word embedding表示，相互之间没什么关系。但是经过与attention score相乘后，V中每个token的向量（即一个单词的word embedding向量），在300维的每个维度上（每一列）上，都会对其他token做出调整（关注度不同）。与V相乘这一步，相当于提纯，让每个单词关注该关注的部分。</div><h2 class="notion-h notion-h1 notion-block-fa8714bc38e6422e851f6b37a0d8d276" data-id="fa8714bc38e6422e851f6b37a0d8d276"><span><div id="fa8714bc38e6422e851f6b37a0d8d276" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fa8714bc38e6422e851f6b37a0d8d276" title="四、Transformer面试问题"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">四、Transformer面试问题</span></span></h2><h3 class="notion-h notion-h2 notion-block-1ac5b3f8555a4d87ae1634356615c85a" data-id="1ac5b3f8555a4d87ae1634356615c85a"><span><div id="1ac5b3f8555a4d87ae1634356615c85a" class="notion-header-anchor"></div><a class="notion-hash-link" href="#1ac5b3f8555a4d87ae1634356615c85a" title="低阶"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">低阶</span></span></h3><details class="notion-toggle notion-block-3597297078fe4be29a66b80297b57300"><summary><b>Transformer是什么？Transformer的结构是什么样的？</b></summary><div><ul class="notion-list notion-list-disc notion-block-6cc16c13543d48bbbaefe6047ce591d9"><li>Transformer是一种基于自注意力机制的神经网络架构，由谷歌于 2017 年在 <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a> 中首次提出。</li></ul><ul class="notion-list notion-list-disc notion-block-0f88b67de42d464b8eccd19275691b7e"><li>Transformer本身还是一个典型的encoder-decoder模型；</li><ul class="notion-list notion-list-disc notion-block-0f88b67de42d464b8eccd19275691b7e"><li>Encoder端由N(原论文中N=6)个相同的模块堆叠而成，除第一个模块外其余模块的输入都是上一个模块的输出；其中每个模块又由多头注意力层（Multi-Head Attention） 以及前馈神经网络层（Feed-Forward Neural Network）组成；</li><li>Decoder端同样由N(原论文中N=6)个相同的大模块堆叠而成，其中每个模块由掩码多头注意力层（Masked<b><b> </b></b>Multi-Head Attention） 、多头编码器-解码器注意力层（Encoder-Decoder attention）以及一个前馈神经网络层（Feed-Forward Neural Network）组成；</li></ul></ul></div></details><details class="notion-toggle notion-block-0fabff02b5684da4b611f8416f2361b4"><summary><b>Transformer中Encoder和Decoder有什么区别？</b></summary><div><ul class="notion-list notion-list-disc notion-block-cf47cfe649b04adab04ac07d7127943a"><li>Decoder 模块比Encoder模块多了一个掩码多头注意力层（Masked<b><b> </b></b>Multi-Head Attention），使用了掩码遮盖的方式防止未来信息的干预</li></ul><ul class="notion-list notion-list-disc notion-block-f1e5feac4d394a15b852d38cb539cedd"><li>Decoder 模块的多头编码器-解码器注意力层（Encoder-Decoder attention）中的K、V都来自Encoder 模块的最终输出，这样能融合来自编码器的信息。</li></ul></div></details><details class="notion-toggle notion-block-e77bf195f5ca42459bf3d60f7bdc808b"><summary><b>Transformer中文本的位置信息是如何获取的？</b></summary><div><div class="notion-text notion-block-3e1aafa85da94a598b48a3306ef13585">原始的 Transformer 论文中提出了使用正弦和余弦函数的固定位置编码，并且位置编码（Positional Encoding）被加入到输入嵌入（Embeddings）中行成了最终的模型输入</div></div></details><details class="notion-toggle notion-block-f5d2230b732a45b5b38ca8fb6d6ab7ff"><summary><b>Transformer中的残差连接和层归一化有什么作用？</b></summary><div><ul class="notion-list notion-list-disc notion-block-9612e966c3b24135a471ceb4eaf71e40"><li><b>残差连接：</b>允许梯度直接流过连接，缓解梯度消失或梯度爆炸的问题，并且这也有助于网络参数的优化提高了训练速度</li></ul><ul class="notion-list notion-list-disc notion-block-1f2b620f9e6849d196864fe5d56bbb37"><li><b>层归一化</b>:</li><ul class="notion-list notion-list-disc notion-block-1f2b620f9e6849d196864fe5d56bbb37"><ol start="1" class="notion-list notion-list-numbered notion-block-d937613658fa4196b923740c60e2a94a"><li>保证了训练过程中各层输入分布的一致性，从而<b>提升训练稳定性</b>。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-75f68fb9df2c432597d67e395ffc2e0e"><li>有助于缓解内部协变量偏移（Internal Covariate Shift）问题，可以<b>加快收敛速度</b>。</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-457d28c65d8844768b708eed3aa9b58d"><li>可以防止网络中的激活值过大或过小，减少了模型训练过程中的梯度消失或爆炸问题，因此可以<b>使用更高的学习率进一步加速训练的同时也提升了模型的稳定性</b>。</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-88d774b3e1be49f382debaa47bbd8c16"><li>归一化层<b>降低了模型对参数初始化的敏感度</b>，减轻了训练初期不适当的参数初始化可能带来的负面影响。</li></ol></ul></ul></div></details><details class="notion-toggle notion-block-1f095722eac646979cf368963ab36fb5"><summary><b>Transformer中的前馈神经网络层使用了什么激活函数？有什么优缺点？</b></summary><div><div class="notion-text notion-block-d866898418b34e49ae83fde20d40dc8d">通常使用ReLU（Rectified Linear Unit）作为激活函数</div><ul class="notion-list notion-list-disc notion-block-a266785c86af475d84183ae2b50bf602"><li><b>优点</b>：</li><ul class="notion-list notion-list-disc notion-block-a266785c86af475d84183ae2b50bf602"><li><b>计算简单</b>：ReLU的计算非常高效，因为它只需要判断输入是否大于0。</li><li><b>缓解梯度消失问题</b>：对于正输入，其导数是常数，这样有助于缓解在深度网络中梯度消失的问题。</li><li><b>稀疏激活</b>：ReLU会导致网络中的神经元输出稀疏的特征表示，可能有利于某些类型的学习。</li></ul></ul><ul class="notion-list notion-list-disc notion-block-4f94b0abbaba4afe9a3880e887ef760c"><li><b>缺点</b>：</li><ul class="notion-list notion-list-disc notion-block-4f94b0abbaba4afe9a3880e887ef760c"><li><b>死亡ReLU问题</b>：如果激活值小于0，ReLU的梯度为0，这会导致神经元不再更新，称为“死亡ReLU”。</li></ul></ul></div></details><details class="notion-toggle notion-block-7b5f202ea1d84da0b3a5a92bdc72802f"><summary><b>Transformer相比于RNN/LSTM，有什么优势？为什么？</b></summary><div><ol start="1" class="notion-list notion-list-numbered notion-block-5532b148db374302a77c0f03c9b209b3"><li><b>并行计算</b>：Transformer 模型能同时处理整个输入序列，而不依赖于先前的计算结果。 RNN和LSTM 当前的计算需要依赖上一步的计算结果。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-9a4d4d24bfec41a7a0d8d06d7eb98098"><li><b>避免长距离依赖问题</b>：RNN 和 LSTM 由于其递归性质，在处理长序列时可能遇到梯度消失或爆炸问题，导致难以学习长距离依赖。Transformer 使用多头自注意力机制（Multi-Head Attention）可以直接捕捉序列内任意两个元素之间的依赖关系，无论它们相隔有多远；Transformer有更好的特征抽取能力</li></ol></div></details><h3 class="notion-h notion-h2 notion-block-478312b7e0be48d8b130f30bbbce5761" data-id="478312b7e0be48d8b130f30bbbce5761"><span><div id="478312b7e0be48d8b130f30bbbce5761" class="notion-header-anchor"></div><a class="notion-hash-link" href="#478312b7e0be48d8b130f30bbbce5761" title="进阶"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">进阶</span></span></h3><details class="notion-toggle notion-block-0548922d9c2b4ccd8d418b0e0f46ba81"><summary><b>Transformer为何使用多头注意力机制？（为什么不使用一个头）</b></summary><div><div class="notion-text notion-block-661ba90a29e24a08b54c3589478c34ce">多头注意力机制核心思想是同时学习数据的不同表示子空间中的信息。多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。并且论文原作者发现这样效果确实好</div></div></details><details class="notion-toggle notion-block-8da9c6e85222427296209db40fb11f4f"><summary><b>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？ （注意和第一个问题的区别）</b></summary><div><div class="notion-text notion-block-a8782d79a8904b0d988941b48651f4a8">使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力</div><div class="notion-text notion-block-3788b19169d74bff991a1096fac8e55c"><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.zhihu.com/question/319339652/answer/730848834">transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 赤乐君的回答 - 知乎</a></b></div></div></details><details class="notion-toggle notion-block-7b90887f1e124eb8abec79f41011583e"><summary><b>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</b></summary><div><div class="notion-text notion-block-d8d60f0efae847bba89ce768cb3ce919">两个向量（矩阵）的点乘表示两个向量（矩阵）的相似度，因此点乘更容易捕捉token之间的关联性和相似性；加法在数学上表达的是元素级的组合，它不考虑向量间的相互对应关系。可以通过高度优化的矩阵计算库来实现，所以点积和加法计算成本大致相当。计算复杂度为 O(d_k*seq_len)</div></div></details><details class="notion-toggle notion-block-0c2362946c404e428ecd1447a21ec018"><summary><b>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</b></summary><div><div class="notion-text notion-block-4d5556914d7c4e73bb2fd3f80fbb88de">原始分数通过点积操作计算得到，点积结果的方差会随着向量的维度增大而增大，而过大的值会导致 softmax 函数进入梯度非常小的区域，这可能会引起梯度消失、降低模型灵活性从而导致模型不稳定；所通过除以<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 的方式来对原始分数进行缩放，从而使模型更加稳定收敛，并且减少梯度消失或梯度爆炸的风险。</div></div></details><details class="notion-toggle notion-block-8c8225e4fda140c58a8b30c4f3e9289f"><summary><b>在 Decoder 模块的掩码多头注意力层（Masked Multi-Head Attention）中，为什么要做Mask操作？是如何做的？在那个步骤做？</b></summary><div><ul class="notion-list notion-list-disc notion-block-548bfabe836d4d89a15cfb98b72b5084"><li>为什么做Mask操作：可以理解为在训练过程中，Decoder 模块的掩码多头注意力层这里输入的是“答案”，但是我们不能让模型直接看见所有答案（这样就不用训练了），所以我们需要对未来的答案进行遮挡隐藏，只放出当前需要用到的词，结合 Encoder 的输出来预测后面的词。</li></ul><ul class="notion-list notion-list-disc notion-block-82f8d87e9b704cf6b5f2b86df712cf59"><li>做Mask操作的位置：在计算出<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>之后，softmax之前做Mask操作</li></ul><ul class="notion-list notion-list-disc notion-block-98421a9a6e5140e281f2c5dc122c118b"><li>怎么做Mask操作：</li><ul class="notion-list notion-list-disc notion-block-98421a9a6e5140e281f2c5dc122c118b"><li>生成一个上三角形的掩码矩阵，该矩阵的上三角部分（包括对角线上方的元素）由负无穷大（或一个非常小的数字，如 -1e9）填充，而其余部分则填充为0。</li><li>将掩码矩阵与<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>计算得到的缩放分数（Scaled Scores）相加，上三角位置对应的数据变成负无穷大（- inf），下三角位置的数据不变；</li><li>通过softmax归一化，就能将负无穷大（- inf）变为 0，得到的这个矩阵即为每个字之间的权重（未来信息的权重被变为 0）</li></ul></ul></div></details><details class="notion-toggle notion-block-edc9755aabe845c39c69535319c54e30"><summary><b>Encoder端和Decoder端是如何进行交互的？</b></summary><div><div class="notion-text notion-block-bc22e5dbb27e43efa61af95b7ce33a8a">编码器-解码器注意力机制是Decoder端与Encoder端交互的关键。它使用Decoder的自注意力层的输出作为查询（Query），使用Encoder的输出作为键（Key）和值（Value）。这允许Decoder的每个位置都能够访问Encoder的整个输入序列的上下文信息。</div></div></details><details class="notion-toggle notion-block-2ebebf556d964ba9a9331c3a76ce38a6"><summary><b>Transformer的并行化体现在哪个地方？Decoder端可以做并行化吗？</b></summary><div><ol start="1" class="notion-list notion-list-numbered notion-block-33a1507718d14fc39ad02d09249ecd3e"><li><b>自注意力机制</b>：在自注意力层中，计算每一个单词对其它所有单词的注意力权重是一个高度并行的操作。因为各个单词之间的这些计算是独立的，所以可以同时进行。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-c7385c489ee6482f8cb1535b4f5e5917"><li><b>批处理</b>：Transformer训练时通常一次性处理整个批次的数据。因为不同序列的自注意力计算相互独立，它们可以并行执行。</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-1946ed21efaf44d4ac424c141241fae8"><li><b>前馈网络</b>：在每个Encoder和Decoder层中，前馈网络对每个位置的数据处理是独立的。这使得对一个序列的所有位置执行前馈网络的计算可以并行进行。</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-26c09ba022e24821a8c314d5e286d352"><li><b>层与层之间的并行</b>：Transformer具有多层结构，当将它们分布于多个计算设备时，不同层的计算可以并行处理。</li></ol><div class="notion-blank notion-block-c1efda949bbf4367966d0c192014c814"> </div><div class="notion-text notion-block-f842f12726224c11ab2c23464398bb46">在Decoder端存在某些限制，主要是因为解码通常需要按顺序生成输出序列，即在生成下一个词之前要考虑之前所有已生成的词的信息。</div></div></details><details class="notion-toggle notion-block-6bfa9f858bed4d22aed32bb790cdbded"><summary><b>Transformer模型中的损失函数是什么？如何进行模型的训练和优化？</b></summary><div><div class="notion-text notion-block-b47db7f4b4854861bd1be04665590235">在Transformer模型中，最常用的损失函数是交叉熵损失（Cross-Entropy Loss），这个损失函数衡量了模型预测的概率分布与真实标签的概率分布之间的差异。</div><div class="notion-text notion-block-166d6ce3c7404f50a4810652dbe01953"><b>模型训练和优化</b>：</div><ol start="1" class="notion-list notion-list-numbered notion-block-4bb6bb0a1c3b4eaeaae7e1ef41e24753"><li><b>初始化模型参数</b>：一般来说，参数是随机初始化的，但要遵循某些特定的初始化策略，如使用Glorot初始化或He初始化。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-a420c9be2702486b961803858f1784f1"><li><b>前向传播</b>：模型接收输入数据，并通过编码器和解码器的各个层，最终给出预测输出。</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-004174aa9a66433dbc9d3f122a9e5e99"><li><b>计算损失</b>：使用交叉熵损失函数比较模型输出与真实标签，计算损失值。</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-063d52f59a0945afaba45d6f4819ceaa"><li><b>反向传播</b>：根据损失函数值用于计算模型参数的梯度。梯度表示损失关于每个参数的局部变化率。</li></ol><ol start="5" class="notion-list notion-list-numbered notion-block-9b1ba3135b0a41f2921e8c9740665624"><li><b>优化器更新参数</b>：使用优化算法（如Adam算法）根据梯度更新模型参数。Adam优化器因其对超参数的选择相对鲁棒，以及内置的适应性学习率特性，经常被用来训练深度学习模型。</li></ol><ol start="6" class="notion-list notion-list-numbered notion-block-b29dbc5f2ffc4c1d872273cac1c50e7d"><li><b>正则化和学习率调整</b>：可能会采用dropout、权重衰减等正则化策略来防止模型过拟合。此外，也会采用学习率调整策略（如学习率预热和衰减）来改进训练过程。</li></ol><ol start="7" class="notion-list notion-list-numbered notion-block-eb300f972fa34ebb9db9569f5e1491f3"><li><b>重复迭代</b>：重复执行前向传播、损失计算、反向传播和参数更新步骤，直到模型收敛，即损失函数不再大幅度减小或达到预定的迭代次数。</li></ol><div class="notion-text notion-block-082011a183c049bcb5c6aa328acece8a">在实际操作中，这个过程会在一个或多个epoch中重复，每个epoch都会处理整个训练数据集。通常会采用mini-batch训练，这意味着不是一次性处理整个数据集，而是将数据集分成许多较小的批次来进行迭代更新。这有助于稳定梯度估计，同时减少内存的要求。</div></div></details><h3 class="notion-h notion-h2 notion-block-1f1e9bccc3154832aafc1acb0f48ed8c" data-id="1f1e9bccc3154832aafc1acb0f48ed8c"><span><div id="1f1e9bccc3154832aafc1acb0f48ed8c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#1f1e9bccc3154832aafc1acb0f48ed8c" title="高阶"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">高阶</span></span></h3><details class="notion-toggle notion-block-2582e597f2ac4f25ace7aab33f89068b"><summary>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</summary><div><div class="notion-text notion-block-2cb349a4afba4fef9f62648ac5805918"><b>学习率</b>：</div><ul class="notion-list notion-list-disc notion-block-6635d5a1eb9041b989f5e563501e03b0"><li>Transformer模型通常采用一种特别的学习率调整策略，称为“学习率预热”（learning rate warmup）。</li></ul><ul class="notion-list notion-list-disc notion-block-ca3c531bd2da444a9fa65efaf6243166"><li>在这种策略中，学习率会在训练开始的几个步骤中线性地增长，达到一个最大值，然后根据一定的规则（例如逐步减少或者按比例下降）随训练步数的增加而降低。</li></ul><ul class="notion-list notion-list-disc notion-block-df4686b188b44c93aac9e9dac9558ad2"><li>一个常见的学习率调度公式如下：<code class="notion-inline-code">lrate = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))</code>，其中<code class="notion-inline-code">d_model</code>是模型的隐藏层维度，<code class="notion-inline-code">step_num</code>是当前的步数，<code class="notion-inline-code">warmup_steps</code>是预热步数。</li></ul><ul class="notion-list notion-list-disc notion-block-c06f5c22749c41468282d0b68ed33be5"><li>这种方法使得模型在训练初期更具探索性，而在后期则集中于稳定和细化学习。</li></ul><div class="notion-text notion-block-879d3a78dac446e99db633607e2cb41e"><b>Dropout</b>：</div><ul class="notion-list notion-list-disc notion-block-53b0bd36e1464dd1860bd07f9a3950dd"><li>Dropout是一个正则化技术，它通过在训练过程中随机“丢弃”一部分神经网络单元来预防过拟合。</li></ul><ul class="notion-list notion-list-disc notion-block-bffaec8ec1a241a4a59d261bb53bf85d"><li>在Transformer模型中，Dropout可以应用在几个不同的位置：</li><ul class="notion-list notion-list-disc notion-block-bffaec8ec1a241a4a59d261bb53bf85d"><li>在词嵌入层后。</li><li>在每个子层的输出上（即自注意力层和前馈网络层之后）。</li><li>在添加位置编码后、提供给Encoder和Decoder层之前。</li><li>在注意力权重上，这通常被称为“注意力Dropout”。</li></ul></ul><ul class="notion-list notion-list-disc notion-block-c592b39602d041d5b20ea51e094c4bf3"><li>Dropout率（即丢弃概率）是一个超参数，通常通过交叉验证来决定。在不同的数据集和任务上可能有所不同，一般范围在0.1到0.3之间。</li></ul><div class="notion-text notion-block-a793fcd5d1d44f9a8b3b46d1d21ba641"><b>Dropout在推理（测试）时的注意点</b>：</div><ul class="notion-list notion-list-disc notion-block-88a4609b38ad485191bddb7b06ea268f"><li>在模型推理或评估时，Dropout应该被禁用，这意味着不应随机丢弃任何单元。</li></ul><ul class="notion-list notion-list-disc notion-block-662f64e2e6a147f4807726638afb63b0"><li>训练中应用的Dropout在推理时被移除，确保了所有的单元都参与前向传播，这可以保证得到稳定的输出。</li></ul><ul class="notion-list notion-list-disc notion-block-6cc9d423c5514c2cb2953bc5104728e6"><li>这通常是通过模型或框架内置的评估模式来自动完成的，比如在PyTorch中，可以通过调用<code class="notion-inline-code">.eval()</code>方法来设置模型为评估模式，禁用Dropout。</li></ul></div></details><details class="notion-toggle notion-block-0344746806484f64acb16c9a53758cac"><summary>为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）</summary><div><div class="notion-text notion-block-e302d6075ec345a69ac697904d9cce7b">在进行多头注意力计算时，对每个头进行降维（即减少每个头处理的特征维度）主要基于以下原因：</div><ol start="1" class="notion-list notion-list-numbered notion-block-054c5bb4475e43a081c8b3e471dc92e3"><li><b>资源优化</b>：<!-- -->多头注意力通过分割原始的特征维度到不同的头中来工作，如果不进行降维，每个头都将操作完整维度的数据，这会导致参数量和计算量剧增。降维可以有效减少每个头的参数数量，使得整个模型更加轻量化，同时便于并行计算。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-2a58f84c2cd24f14987270375c9e299a"><li><b>专注力分散</b>：<!-- -->降维可以让每个头专注于输入的不同子空间，捕获输入中不同方面的信息。如果不降维，则每个头操作的信息冗余度会增加，可能导致学习的特征表示不够多样化。</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-5eda31193d94458ea570c64e5d945ab8"><li><b>模型表达能力</b>：<!-- -->通过降维并分配到多个不同的头，模型可以并行学习输入序列的多个方面，增加了模型的表达能力，并允许模型在较低的维度上更深入地学习数据的细节。</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-a3abb0d47c66432fa0f37b55ccb014e6"><li><b>防止过拟合</b>：<!-- -->由于在降维之后每个头学习的参数更少，这可以起到一定的正则化效果，减小过拟合风险。</li></ol></div></details><details class="notion-toggle notion-block-84b5e348c7e54be38ec730d9b5104314"><summary>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</summary><div><div class="notion-text notion-block-3ce348e98d5f43dcb559dfeff9f18955"><b>优点</b>：</div><ul class="notion-list notion-list-disc notion-block-e2344a9059f54064a13c0465431dbb06"><li><b>可解释性强</b>：正弦和余弦位置编码可以通过可变波长的函数来捕获位置和相对位置的信息，这使得模型能够推断出词项之间的距离关系。</li></ul><ul class="notion-list notion-list-disc notion-block-6972d8b091cd46aa8ee1d3218153dcab"><li><b>泛化能力</b>：特别是固定的三角函数位置编码不依赖于特定的序列长度，因此模型可以处理比训练时见过的序列更长的序列。</li></ul><ul class="notion-list notion-list-disc notion-block-8f5662ee8b7846ed9f83a0b062a7fcd4"><li><b>并行计算</b>：位置编码允许模型继续在各个位置同时处理序列的所有元素。</li></ul><div class="notion-text notion-block-b1a75faae4cb43be81b88c8dc9d51b7a"><b>缺点</b>：</div><ul class="notion-list notion-list-disc notion-block-76e56947695d46c9a06f709fc78eb288"><li>当处理的序列长度超过训练数据中的长度时，学习得到的位置编码可能无法很好泛化。</li></ul><ul class="notion-list notion-list-disc notion-block-31c8f690c98243cdaa71fdfd68b4db07"><li>虽然固定的三角函数位置编码有助于模型理解相对位置信息，但它们不包含实际的序列位置，模型可能需要额外的层次和参数来学习更复杂的位置相关模式。</li></ul></div></details><details class="notion-toggle notion-block-dc90712e8fb9488fb822221b969d7765"><summary>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</summary><div><ol start="1" class="notion-list notion-list-numbered notion-block-d5317a9360de4e79845eae1dd89a960f"><li><b>相对位置编码</b>:</li><ol class="notion-list notion-list-numbered notion-block-d5317a9360de4e79845eae1dd89a960f"><ul class="notion-list notion-list-disc notion-block-6d217e2456c947e8a7cdd5ff1bbb1cb0"><li>与绝对位置编码不同，相对位置编码关注的是元素间的相对距离而不是它们的绝对位置。</li></ul><ul class="notion-list notion-list-disc notion-block-85c4d9039dc245c8bb2a0a0fc527fd5a"><li>优点：它能较好地捕捉元素间的局部关系，并且能更好地处理较长的序列。</li></ul><ul class="notion-list notion-list-disc notion-block-56e05c46f8c44fc9ac2f40edd5d192e6"><li>缺点：相对位置关系的建模可能会更加复杂，需要模型学习额外的参数。</li></ul></ol></ol><ol start="2" class="notion-list notion-list-numbered notion-block-662efe8be2e7492b9308c95757ba1b3b"><li><b>索引位置编码</b>:</li><ol class="notion-list notion-list-numbered notion-block-662efe8be2e7492b9308c95757ba1b3b"><ul class="notion-list notion-list-disc notion-block-688e417e41694751889ff6973aa933f2"><li>即为序列中的每个位置指定一个唯一的索引，通常用作查找表（embedding table）。</li></ul><ul class="notion-list notion-list-disc notion-block-7ce595aeae134a11ae9844662ee2fd07"><li>优点：容易实现，并且可以直接学习位置间的复杂函数关系。</li></ul><ul class="notion-list notion-list-disc notion-block-ec2e69a9d2a14129b61dae8f3e803a04"><li>缺点：通常不适用于超出训练集长度范围外的序列。</li></ul></ol></ol><ol start="3" class="notion-list notion-list-numbered notion-block-69644d82ef9c47aeb79330d99778faeb"><li><b>学习位置编码</b>:</li><ol class="notion-list notion-list-numbered notion-block-69644d82ef9c47aeb79330d99778faeb"><ul class="notion-list notion-list-disc notion-block-2c9b23f17bf7439eb5f8ffc48af1c37e"><li>通过神经网络学习位置的表示，与输入的词嵌入一同训练。</li></ul><ul class="notion-list notion-list-disc notion-block-513ed6ab444c4584baedb5746e0d2d2d"><li>优点：可以自动学习复杂的序列模式。</li></ul><ul class="notion-list notion-list-disc notion-block-9ea11bc67e984faa97e59e9b0f735f1c"><li>缺点：如果序列非常长，可能需要很多数据才能有效地学习位置信息。</li></ul></ol></ol><ol start="4" class="notion-list notion-list-numbered notion-block-e76739075c4f4a7699dbf33dfee81669"><li><b>Transformer XL的相对位置编码</b>:</li><ol class="notion-list notion-list-numbered notion-block-e76739075c4f4a7699dbf33dfee81669"><ul class="notion-list notion-list-disc notion-block-817a2eb7400f4b049b66fa505531b2c1"><li>这种方法将基于内容的注意力和基于位置的注意力分开，并且引入了一个可学习的相对位置编码。</li></ul><ul class="notion-list notion-list-disc notion-block-3b33f95645c5403380b8c421c35de4f4"><li>优点：它使得模型可以处理非常长的序列数据，并且能够在长文本上捕获依赖性。</li></ul><ul class="notion-list notion-list-disc notion-block-6a34366490f94c838a81d9459aa6928d"><li>缺点：其结构相对复杂，实现起来比标准的Transformer更具挑战性。</li></ul></ol></ol><ol start="5" class="notion-list notion-list-numbered notion-block-43e08d2f02e1494c8b9bbd5aac74c727"><li><b>ALBERT的位置编码共享（position encoding sharing）</b>:</li><ol class="notion-list notion-list-numbered notion-block-43e08d2f02e1494c8b9bbd5aac74c727"><ul class="notion-list notion-list-disc notion-block-83fa676a8f6b439581a50fa9c532e8b1"><li>在这种方法中，多个层之间共享位置编码，减少了需要学习的参数。</li></ul><ul class="notion-list notion-list-disc notion-block-63a2d2dcaca444bd91c06a90b9441289"><li>优点：减少了模型大小，提高了参数效率。</li></ul><ul class="notion-list notion-list-disc notion-block-0af5b886902342bc862421d3489e1585"><li>缺点：可能限制了模型捕捉不同层特定位置信息的能力。</li></ul></ol></ol><ol start="6" class="notion-list notion-list-numbered notion-block-48aebab48485438f8c9a05d97423f34a"><li><b>可分离位置编码</b>:</li><ol class="notion-list notion-list-numbered notion-block-48aebab48485438f8c9a05d97423f34a"><ul class="notion-list notion-list-disc notion-block-85c3230d39f74392a712fc8d2b5f02ae"><li>在这种方法中，位置编码被设计成可以与词嵌入独立处理，然后再整合在一起。</li></ul><ul class="notion-list notion-list-disc notion-block-4e283cce9ad04324ab96888c7e5a7814"><li>优点：提供了更大的灵活性，不同的任务可以使用不同的位置编码策略。</li></ul><ul class="notion-list notion-list-disc notion-block-238f43908084400f8e3a44fceb77eff4"><li>缺点：可能会增加模型的复杂性和训练难度。</li></ul></ol></ol></div></details><details class="notion-toggle notion-block-f2e9edba6ef7485895c98b67b7a686d7"><summary>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</summary><div><ol start="1" class="notion-list notion-list-numbered notion-block-d3222a26b9544d69b0dfd539c25e4d04"><li><b>独立性</b>：<!-- -->LayerNorm 对每一个样本独立进行归一化，计算的是样本内部特征的均值和方差。相较之下，BatchNorm 在一个小批量的数据上进行归一化，计算的是跨整个小批量数据的特征均值和方差。在处理序列化数据时，每个样本序列的长度可能不同，这使得BatchNorm难以应用，而LayerNorm并不受此影响。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-c5eaf32f86fa4764ab95ca72a80bcf30"><li><b>计算顺序</b>：<!-- -->在序列模型中，由于序列的动态长度和模型解码过程的自回归特性，BatchNorm在时间步之间的归一化将会引入上一个时间步的信息，这可能导致信息泄漏。LayerNorm 限制在单个样本内部，不会出现这种情况。</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-a38749f8c7f94bad884348d7e80da49e"><li><b>在线性变换之后</b>：<!-- -->LayerNorm 通常紧跟在多头自注意力（MultiHead Attention）和前馈（Feed-Forward）层的线性变换之后，而不是像BatchNorm那样在激活函数之前。</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-5e122fae93964f1d80de80f895c89268"><li><b>稳定性</b>：<!-- -->在Transformer模型中，训练的是大量动态序列数据。LayerNorm由于其对每个样本独立操作的特性，更适合这种场景，能够为模型提供额外的稳定性。</li></ol><div class="notion-text notion-block-1f5b10edf46b40ad906f184588593ccd"><b>LayerNorm在Transformer中的位置</b>：<!-- -->在Transformer的架构中，LayerNorm通常放置在以下两个地方：</div><ul class="notion-list notion-list-disc notion-block-f5d1c6bd75654e5485be1709c3a76d4f"><li><b>在每个子层（自注意力与前馈网络）的输出之后，并在残差连接（&quot;add &amp; norm&quot;）之前</b>。这意味着，LayerNorm 是在将子层的输出与输入进行相加前应用的。正是这种&quot;add &amp; norm&quot;的组合提供了模型深度连接的稳定性，允许训练深层的Transformer网络。</li></ul><ul class="notion-list notion-list-disc notion-block-5c9cc38e506a42d5b4c2a7354ad0bdb1"><li>一些变种的Transformer可能会放置在子层之后直接应用LayerNorm，而将残差连接的加法放在归一化之后。</li></ul></div></details><details class="notion-toggle notion-block-ea96a3796f3347af952a970ef879a3d3"><summary>简答讲一下BatchNorm技术，以及它的优缺点。</summary><div><div class="notion-text notion-block-32bb7f4cc0d84149a49346d07dda93f4">Batch Normalization（BatchNorm）是一项在深度神经网络中广泛使用的技术，旨在通过调整层输入的分布来加速训练过程和提高模型的性能。它首次被介绍是在2015年的论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》中。</div><div class="notion-text notion-block-e0c24782336e443f908f273883e7c370"><b>工作原理</b>：<!-- -->BatchNorm 对每个小批量数据（batch）执行以下步骤：</div><ol start="1" class="notion-list notion-list-numbered notion-block-08835158e25d4856b8d22f9f4df340fa"><li><b>计算均值</b>：计算当前批次内数据的均值。</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-3303ca0a75544c2886036a870f5af548"><li><b>计算方差</b>：计算当前批次内数据的方差。</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-f686b75b70eb49c4bcb4b3e181d3b250"><li><b>归一化</b>：使用上述均值和方差对数据进行归一化，确保数据符合标准正态分布（即均值为0，方差为1）。</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-e72ca52b0faf4b249b93dbd3812a11c9"><li><b>缩放和偏移</b>：对归一化的数据进行可学习参数的缩放和偏移（重参数化），以保持网络的表达能力。</li></ol><div class="notion-text notion-block-077196c7908749bf9070a9a10db80b3b"><b>优点</b>：</div><ul class="notion-list notion-list-disc notion-block-f572fb00fa0e4d7f933c26e4950a10a2"><li><b>加速收敛</b>：通过减少不同层分布的差异，BatchNorm有助于加快模型训练的收敛速度。</li></ul><ul class="notion-list notion-list-disc notion-block-38eae71df4d94648abf9a0d7300144de"><li><b>允许更高的学习率</b>：由于提供了稳定性，可以使用更高的学习率而不致于训练过程发散。</li></ul><ul class="notion-list notion-list-disc notion-block-813b2bb49c2e49a5a4e0c682dc2420f3"><li><b>缓解梯度消失问题</b>：规范化过程中的归一化可以减轻梯度消失的问题，使得更深层网络的训练成为可能。</li></ul><ul class="notion-list notion-list-disc notion-block-019cff2849d643069b2211e1a49a9542"><li><b>充当正则化</b>：BatchNorm 在一定程度上可以起到正则化效果，可能降低模型对于过拟合的倾向。</li></ul><div class="notion-text notion-block-fabb92facc3e4c36855964d19fc6b924"><b>缺点</b>：</div><ul class="notion-list notion-list-disc notion-block-d605c6f4aaed499a926ed90e788b502f"><li><b>依赖于批次大小</b>：BatchNorm的归一化效果取决于批次的大小，对小批次数据可能效果不佳。</li></ul><ul class="notion-list notion-list-disc notion-block-27c87f556a044ba2bfb70dfcf28f8954"><li><b>对RNN的支持欠佳</b>：涉及时间依赖性的数据处理（如RNN中）使用BatchNorm可能会导致问题，因为同一层的不同时间步可能会有不同的统计特性。</li></ul><ul class="notion-list notion-list-disc notion-block-c5123ad6476d4a9582a7411e617586ae"><li><b>可能抑制某些网络特性</b>：比如风格迁移学习中，BatchNorm可能会去除某些图像特有的风格特征。</li></ul><ul class="notion-list notion-list-disc notion-block-37026f6483524557acb3a9a73f51955e"><li><b>推理阶段需要调整</b>：在进行模型推理时（模型上线后使用），需要计算整个训练集的均值和方差用于替换批次统计量，此过程称为BatchNorm的归一化参数固化。</li></ul></div></details><details class="notion-toggle notion-block-431357a3c6bd4905a0a62ada88f56b6d"><summary>简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</summary><div><div class="notion-text notion-block-deae5e3e0e2545a38e14818c7924b181">WordPiece模型和Byte Pair Encoding（BPE）都是一种用于文本处理中的子词（subword）分割算法。这些算法的主要应用在于自然语言处理中的词汇分割，尤其是在语言模型预训练和机器翻译中。</div><div class="notion-text notion-block-17cb309665b94ef8bcdcefef2edde276"><b>Byte Pair Encoding (BPE)</b>:</div><ul class="notion-list notion-list-disc notion-block-e8968cf06e224ef89cc15af759ca74c3"><li>BPE 最初来源于数据压缩技术，后来被引入到NLP中用来解决OOV（Out-of-Vocabulary）问题。它的基本思想是将常见的字符对（byte pairs）合并成单个符号。</li></ul><ul class="notion-list notion-list-disc notion-block-5c5e336a8a4144ebb5a9632b26a74baa"><li>在NLP中，BPE通过统计分析语料库中单词之间共现的频率高的字符对并逐步替换这些字符对来生成常见的子词。</li></ul><ul class="notion-list notion-list-disc notion-block-6775d6d9d70344d287b2c7d347ada64e"><li>例如，对于“lower”和“newest”，如果“er”是常见的字符对，则这两个单词可以被分割为“low er”和“new est”，在迭代过程中“er”可能就会被合并成一个新的单独符号。</li></ul><ul class="notion-list notion-list-disc notion-block-f578e3df74ec4e3596c0eb0221b1b55e"><li>BPE允许模型通过有限的词汇来处理未知或罕见的单词，提升了模型对词汇表外单词的泛化能力。</li></ul><div class="notion-text notion-block-ba159d2884b04079887b44c2563e1e43"><b>WordPiece Model</b>:</div><ul class="notion-list notion-list-disc notion-block-8a2bb497a78844d4b889da07f80bca50"><li>WordPiece 也是一种用于生成子词的算法，与BPE类似，但它采用了不同的合并策略。</li></ul><ul class="notion-list notion-list-disc notion-block-6c2fb2686bd74c919e36e3be51d990e5"><li>WordPiece不仅仅基于字符对的频率，还考虑了生成子词后的语言模型得分，以此来选择合并哪些字符对。</li></ul><ul class="notion-list notion-list-disc notion-block-ea03ad9d1244400daafc81f348eaa625"><li>这意味着WordPiece在合并时不只考虑局部频率信息，还尝试优化整体概率，使得生成的词表更适合后续的任务。</li></ul><ul class="notion-list notion-list-disc notion-block-491cbbc3c64a4fe5819c9fb7ebbf778f"><li>Google在其机器翻译系统中使用WordPiece，BERT模型中也使用WordPiece作为其分词机制。</li></ul><div class="notion-text notion-block-efe3bea478dc446ea9b39b0b2106560c">这两种方法都被广泛应用在多种语言模型中，尤其是那些基于Transformer架构的模型。例如，在预训练的BERT模型中，就使用了WordPiece算法来分词，并且它对于处理多语言数据，特别是那些词汇组合丰富且语言结构复杂的任务，表现出了卓越的效果。而GPT-2、GPT-3等模型，采用的则是一种类似于BPE的子词分割算法。这些分词方法有效地支持了模型处理长文本序列和理解语言中词与词之间的关系，并在各种自然语言理解和生成任务中得到了应用。</div></div></details><details class="notion-toggle notion-block-5fab31c75c93499e8cfeb13aba5f7de7"><summary>引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</summary><div><div class="notion-text notion-block-4336b1b50f30486d9e031710d1487845">BERT确实使用了掩码（masking）技巧，但它在预训练阶段所使用的掩码和Transformer Decoder中的掩码有不同的目的和实现方式。</div><div class="notion-text notion-block-f9c339dce4b94654a36560a15807761d"><b>BERT的学习任务</b>：<!-- -->BERT的预训练包括两个主要的任务：掩码语言模型（Masked Language Model, MLM）和下一个句子预测（Next Sentence Prediction, NSP）。掩码语言模型中，BERT随机选择输入序列的一部分token进行掩码处理，具体来说，它会将这部分token替换为一个特殊的<code class="notion-inline-code">[MASK]</code>标记，然后模型被训练为预测这些被掩盖的原始token。</div><div class="notion-text notion-block-3db4d82bebaf47be98b6ea1dcad18166"><b>掩码（Masking）</b>：</div><ul class="notion-list notion-list-disc notion-block-778d161cf15d42f092fed5b77a7ec8bf"><li>传统的Transformer模型在Decoder的自注意力模块使用掩码来实现自回归行为，在生成每个token时，确保只能注意到前面的token，预防信息泄露（即看到未来的token）。这是通过在attention score计算之后、softmax之前将未来位置的评分设置为负无穷的方式实现。</li></ul><ul class="notion-list notion-list-disc notion-block-46fa965dafd94375b5c3247b5988b140"><li>BERT由于是一个双向模型（Bi-directional），在预训练的MLM任务中，不应该阻止一个token注意到它后面的token。如果BERT采取了Transformer Decoder那样的掩码技巧，那么就变成了一个单向模型，无法实现其设计上的双向特性。</li></ul><div class="notion-text notion-block-e9014a6cb5a54ed2b88fcffcebf24eeb">因此，BERT在attention计算中没有进行Decoder端那样的屏蔽操作是有意为之，目的是为了让预训练的模型能够整合上下文中的双向信息。BERT的目标是预测出被掩盖掉的token，为了达到这个目标，并不需要对注意力得分进行屏蔽。相反，通过允许模型看到整个输入序列的所有token，BERT能够更好地学习语言的双向表征，这是其在大范围下游任务中取得成功的关键。</div></div></details><h2 class="notion-h notion-h1 notion-block-04c2342ccf664e57afd2836871abe603" data-id="04c2342ccf664e57afd2836871abe603"><span><div id="04c2342ccf664e57afd2836871abe603" class="notion-header-anchor"></div><a class="notion-hash-link" href="#04c2342ccf664e57afd2836871abe603" title="五、推荐阅读"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">五、推荐阅读</span></span></h2><ol start="1" class="notion-list notion-list-numbered notion-block-89c60b4cb1da4ad68589f77ebccfe238"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://wmathor.com/index.php/archives/1438/">Transformer 详解</a></b></li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-3898ef2eed264c858408a32ca15f3bee"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://terrifyzhao.github.io/2019/01/11/Transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3.html">Transformer 模型详解</a></b></li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-fefd71f37f40463c88e02b0b6bb9236a"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8Transformer%E9%9D%A2%E8%AF%95%E9%A2%98.md"><b>史上最全Transformer面试题</b></a></li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-d48e77c7771347a1aa4b092b6d308d08"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://zhuanlan.zhihu.com/p/338817680"><b>Transformer模型详解（图解最完整版）</b></a></li></ol><ol start="5" class="notion-list notion-list-numbered notion-block-8c7a688c583646feae628b9da5bed4a0"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3"><b>Dissecting BERT Part 1: The Encoder</b></a></li></ol><ol start="6" class="notion-list notion-list-numbered notion-block-a014534ca59d4a68b52aeda0b6f3f716"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://wmathor.com/index.php/archives/1453/"><b>Transformer 中的 Positional Encoding</b></a></li></ol><ol start="7" class="notion-list notion-list-numbered notion-block-083369cadbae490ca158664ed80cf2a8"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer——哈佛大学</a></b></li></ol><ol start="8" class="notion-list notion-list-numbered notion-block-d86f56df050d466ea819ebbbb5e92d1d"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://jalammar.github.io/illustrated-transformer/"><b>The Illustrated Transformer——Jay Alammar</b></a></li></ol><ol start="9" class="notion-list notion-list-numbered notion-block-8a3d17ff29b94aafaf85ac43561d7953"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://zhuanlan.zhihu.com/p/48508221"><b>详解Transformer （Attention Is All You Need）</b></a></li></ol><ol start="10" class="notion-list notion-list-numbered notion-block-af22b41493d04a898998f31c8f87c42a"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://zhuanlan.zhihu.com/p/37601161"><b>深度学习中的注意力模型（2017版）——张俊林</b></a></li></ol><ol start="11" class="notion-list notion-list-numbered notion-block-ebb1f415861b4274808bdde8d2a96951"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://zhuanlan.zhihu.com/p/69290203">Transformer结构及其应用详解--GPT、BERT、MT-DNN、GPT-2</a></b></li></ol><ol start="12" class="notion-list notion-list-numbered notion-block-089cd665359143519c36c05be8be6b53"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.zhihu.com/question/347678607">如何理解Transformer论文中的positional encoding，和三角函数有什么关系？</a></b></li></ol><ol start="13" class="notion-list notion-list-numbered notion-block-b4562dd974bd431fa698c69aba5062ac"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://zhuanlan.zhihu.com/p/148656446"><b>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！</b></a></li></ol><ol start="14" class="notion-list notion-list-numbered notion-block-b22b0678c2cd4ff8a72faccbd3ff3417"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.zhihu.com/question/319339652/answer/730848834">transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 赤乐君的回答 - 知乎</a></b></li></ol><ol start="15" class="notion-list notion-list-numbered notion-block-f9ccf6826c01457781fed49a84e9bff5"><li><b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://zhuanlan.zhihu.com/p/54743941">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较——张俊林</a></b></li></ol></div></main></div>]]></content:encoded>
        </item>
    </channel>
</rss>